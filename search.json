[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "M.S. Student in Social Data Analytics & Research at the University of Texas at Dallas.",
    "section": "",
    "text": "Hello! My name is Jhanvi Kannan, and I graduated from the University of Texas at Dallas in May 2023 with my Bachelor’s in Science in Business Administration, with a concentration in Innovation & Entrepreneurship. During my undergrad degree, I focused on many different elements of being an entrepreneur, such as sales and marketing, as well as testing the viability of new products and the estimated demand for it in a market.\nWith my masters degree, I am hoping to fine tune my research skills and use data analysis to help improve my abilities to be a great entrepreneur. Using this masters, I would be well equipped to research the demand of new technology needed in a community, and help bring that technology forth. I aspire to help push the world to grow in a well-mannered and ethical way by focusing on what people need to make their lives better in a day-to-day environment.\nContact Me: jhanvi.kannan@utdallas.edu\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Assignment 1: Qualtrics Survey",
    "section": "",
    "text": "Link to survey: https://utdallas.qualtrics.com/jfe/form/SV_8qYDz8fe1fd8nZQ\nSurvey Analysis:\n\nThe survey is structured linearly to collect data and gauge responses on how frequently the sample population watch movies, and what content in movies they prefer. This is asked before addressing the demographics of the sample population.\nThe questionnaire is composed of multiple choice questions (objective questions), as well as Likert Scales questions. This is used to collect data based on the opinions of the sample population.\nThe questions are ordered by starting with a Likert Scales question, that then continues into multiple choice questions based on the opinions of the topic. The questionnaire concludes by collecting information on the sample population’s demographics.\n\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#jhanvi.kannanutdallas.edu",
    "href": "index.html#jhanvi.kannanutdallas.edu",
    "title": "jhanvi-kannan.github.io",
    "section": "jhanvi.kannan@utdallas.edu",
    "text": "jhanvi.kannan@utdallas.edu"
  },
  {
    "objectID": "index.html#assignment-1-qualtrics-survey",
    "href": "index.html#assignment-1-qualtrics-survey",
    "title": "jhanvi-kannan.github.io",
    "section": "Assignment 1: Qualtrics Survey",
    "text": "Assignment 1: Qualtrics Survey\nLink to survey: https://utdallas.qualtrics.com/jfe/form/SV_8qYDz8fe1fd8nZQ\nSurvey Analysis:\n\nThe survey is structured linearly to collect data and gauge responses on how frequently the sample population watch movies, and what content in movies they prefer. This is asked before addressing the demographics of the sample population.\nThe questionnaire is composed of multiple choice questions (objective questions), as well as Likert Scales questions. This is used to collect data based on the opinions of the sample population.\nThe questions are ordered by starting with a Likert Scales question, that then continues into multiple choice questions based on the opinions of the topic. The questionnaire concludes by collecting information on the sample population’s demographics.\n\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html#assignment-1-qualtrics-survey",
    "href": "about.html#assignment-1-qualtrics-survey",
    "title": "Assignment 1: Qualtrics Survey",
    "section": "",
    "text": "Link to survey: https://utdallas.qualtrics.com/jfe/form/SV_8qYDz8fe1fd8nZQ\nSurvey Analysis:\n\nThe survey is structured linearly to collect data and gauge responses on how frequently the sample population watch movies, and what content in movies they prefer. This is asked before addressing the demographics of the sample population.\nThe questionnaire is composed of multiple choice questions (objective questions), as well as Likert Scales questions. This is used to collect data based on the opinions of the sample population.\nThe questions are ordered by starting with a Likert Scales question, that then continues into multiple choice questions based on the opinions of the topic. The questionnaire concludes by collecting information on the sample population’s demographics.\n\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Assignment 2.html",
    "href": "Assignment 2.html",
    "title": "Assignment 2: Google Trends Data",
    "section": "",
    "text": "I used Google Trends and the gtrendsR Package to analyze data regarding the terms “Trump, Biden and Election”.\nWhen analyzing these terms on Google Trends, I noticed that the term “election” peaked on November 6, 2022, while the terms “Trump” and “Biden” stayed relatively stable all throughout 2022 and 2023. The term “Trump” had several hits around April and August of 2023, but it is safe to infer that it was not in correlation with the term “election”, as this was likely due to Trump’s presence in court cases at the time.\n\n\n\n\n\nUsing the gtrends package in R Studio yielded different results. The graph displayed by R studio shows that “Trump” was a popular term searched since 2005, but the term started picking up in search frequency around the 2016 election time. His next peak was in 2023, again likely due to his ongoing court cases. The term “election”, however was the most consistent in its peak, spiking every 4 years and then dying back down. It hit its all time peak this year in 2023, however, reaching a search frequency of 100 search hits. The term “Biden” had the lowest search hits, and had very minimal spikes in his trends.\n\n\n\n\n\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Assignment 3.html",
    "href": "Assignment 3.html",
    "title": "Assignment 3: Quanteda",
    "section": "",
    "text": "Biden-Xi Summit\nThe Biden-Xi summit was a virtual summit between U.S. President Joe Biden and Chinese President Xi Jinping, and was held on November 15 and 16, 2021. This summit was a critical diplomatic meeting between the leaders of the United States and China and aimed to address a range of important issues in the U.S.-China relationship, such as bilateral relations, economic and trade issues, human rights and climate change, regional and cyber security, and global issues such as COVID 19.\nWe used the provided R code and the Quanteda package to perform a text analysis on Twitter data regarding the Biden-Xi Summit. The following is what was identified from the analysis:\nThis line of code (head(tweet_dfm)) helps provide insight of the document terms in the data matrix.\n\n\n\n\n\nThis line of code (head(toptag, 10)) extracts the top 10 frequently used hashtags in the data matrix. As you can see, these hashtags are generically focused on the countries and presidents that the summit revolves around, but also touches on some global issues, such as COVID-19, drug usage, and the Uyghur Genocide.\n\n\n\n\n\nThese lines of code (head(tag_fcm)) explores the relationship between the hashtags, and how they are used together. This image shows the analysis on the first few rows of hashtags, and how they correspond together.\n\n\n\n\n\nThis network plot shows visually the relationship between the hashtags, and how they correspond with each other.\n\n\n\n\n\nThis line of code (head(topuser, 20)) extracts the top 20 frequent Twitter usernames found in the data matrix.\n\n\n\n\n\nThis image shows a network plot of a subset of Twitter usernames that were pulled from the data matrix, to show the visual relationship between usernames and their frequency of co-occurence in the matrix.\n\n\n\n\n\n\n\nU.S. Presidential Inaugural Speeches\nTo analyze Presidential Inaugural Speeches, I used the Quanteda package in R Studio to generate visualizations. The following is what I identified from my analysis:\n\n\n\n\n\nTo retrieve this image, we used the code ’kwic(tokens(data_corpus_inaugural_subset), pattern = “american”) %&gt;%\ntextplot_xray().\nBy using this code, we perform a keyword-in-context analysis and specifically search for the word “american” in the data matrix. It then generates the visualization so we can see the frequency in which the term “american” was used in each president’s inaugural speeches.\n\n\n\n\n\nWe use a similar code to derive this visualization, except we add in the functions to generate for the keywords “people” and “communist” as well. This visualization shows us how frequently each word was used in each president’s inaugural addresses, and how they compare to each of the other words as well.\nOver time and between presidents, the usage of the term “american” varied, but the usage of the term itself was not that high. The usage of the term “people” was a lot higher, and was used frequently in every president’s inaugural address.\n\n\nWhat is Wordfish?\nWordfish is a Quanteda Package function used in R Studio to analyze the relationship between words in documents. It is often used for text scaling, and will show the associated strength of each word within a document. To use this function, use ‘textstat_wordfish()’.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Assignment 6.html",
    "href": "Assignment 6.html",
    "title": "Assignment 6: Webscraping",
    "section": "",
    "text": "The textmining01.r code is a script for performing text mining and generating a word cloud from downloaded text data regarding Martin Luther King Jr.’s “I have a Dream” speech. The script downloads data using the “htmlTreeParse” function and stores it as a variable. The text is then pre-processed and vectorized, with the text being converted to all lower case, number and punctuation stripped away, and a matrix formed to represent the frequencies. The matrix is shown below:\n\n\n\n\n\nThe “wordcounts” variable is then used to order these frequencies in descending order, shown below:\n\n\n\n\n\nWith this, we can then formulate Word Clouds with our processed text data. The larger the word is presented in the Word Cloud, the more frequent it is presented in the speech.\n \n\n\nNext, we will conduct a similar analysis on Winston Churchill’s speech “The Finest Hour”. We will use the following full script to analyze the text.\n\n\nThe Word Cloud is presented below:\n\n\n\n\n\nHere we can see from our text analysis that words such as “war”, “france” and “french”, and “great” are very frequent in the speech."
  },
  {
    "objectID": "Assignment 6.html#their-finest-hour--winston-churchill",
    "href": "Assignment 6.html#their-finest-hour--winston-churchill",
    "title": "Assignment 6: Webscraping",
    "section": "",
    "text": "Next, we will conduct a similar analysis on Winston Churchill’s speech “The Finest Hour”. We will use the following full script to analyze the text.\n\n\nThe Word Cloud is presented below:\n\n\n\n\n\nHere we can see from our text analysis that words such as “war”, “france” and “french”, and “great” are very frequent in the speech."
  },
  {
    "objectID": "Assignment 7.html",
    "href": "Assignment 7.html",
    "title": "Assignment 7: Gov. Data & Parallel Processing",
    "section": "",
    "text": "With the R Scripts in govdata01.r and parallel01.r, we are attempting to download data sets from government info and clean the variables, exporting them out into individual data sets in PDF format. Unfortunately, however, my script repeatedly failed to download the PDFs of the government info data, as R kept informing me it could not read the table due to there being more columns than column names.\nHowever, I attempted the given exercise, which was to process data for \"118th Congress Congressional Hearings in Committee on Foreign Affairs?\" but a similar problem persisted. Due to this, I was unable to run a parallel computation using the parallel01.r script."
  },
  {
    "objectID": "Assignment6.html",
    "href": "Assignment6.html",
    "title": "Assignment 6: Webscraping",
    "section": "",
    "text": "The textmining01.r code is a script for performing text mining and generating a word cloud from downloaded text data regarding Martin Luther King Jr.’s “I have a Dream” speech. The script downloads data using the “htmlTreeParse” function and stores it as a variable. The text is then pre-processed and vectorized, with the text being converted to all lower case, number and punctuation stripped away, and a matrix formed to represent the frequencies. The matrix is shown below:\n\n\n\n\n\nThe “wordcounts” variable is then used to order these frequencies in descending order, shown below:\n\n\n\n\n\nWith this, we can then formulate Word Clouds with our processed text data. The larger the word is presented in the Word Cloud, the more frequent it is presented in the speech.\n \n\n\nNext, we will conduct a similar analysis on Winston Churchill’s speech “The Finest Hour”. We will use the following full script to analyze the text.\n\n\nThe Word Cloud is presented below:\n\n\n\n\n\nHere we can see from our text analysis that words such as “war”, “france” and “french”, and “great” are very frequent in the speech."
  },
  {
    "objectID": "Assignment6.html#their-finest-hour--winston-churchill",
    "href": "Assignment6.html#their-finest-hour--winston-churchill",
    "title": "Assignment 6: Webscraping",
    "section": "",
    "text": "Next, we will conduct a similar analysis on Winston Churchill’s speech “The Finest Hour”. We will use the following full script to analyze the text.\n\n\nThe Word Cloud is presented below:\n\n\n\n\n\nHere we can see from our text analysis that words such as “war”, “france” and “french”, and “great” are very frequent in the speech."
  },
  {
    "objectID": "Assignment8.html",
    "href": "Assignment8.html",
    "title": "Assignment 8: Census & Spatial Data",
    "section": "",
    "text": "In the spatialdata01.r code, we explore the median age by state in 2019 using an API Census key.\nThe code loads American Community Survey (ACS) 2019 variables using the “tidycensus::load_variables” function. It also loads ACS 2019 profile variables. The “get_acs” function retrieves ACS data for median age by state in 2019. The data is obtained at the state level, and the resulting graph is shown below.\n\n\n\n\n\nThe final map visualizes the median age by state in 2019, providing a geographical representation of the data. Each state is shaded according to its median age, with the darker colors indicating a higher median age and the lighter colors indicating a lower median age.\n\n\nThe given exercise in the spatialdata01.r file was to test the code to download 2009 and 2020 data as well and compare them. Rewriting the code to download 2009 data was simple, and the visual is provided below. However, downloading the 2020 data was not possible, as an error occurred reading “Error in get_acs():! The regular 1-year ACS for 2020 was not released and is not available in tidycensus.”\n\n\n\n\n\nAt first glances, there did not seem to be many differences between the 2009 and 2019 data, but there are definitely some changes present. For example, the median age in Montana has decreased over the decade, as well as in Minnesota, Pennsylvania, North Dakota and New Jersey. Realistically, the only place we see the median age increasing in is in the U.S. Territory."
  },
  {
    "objectID": "Assignment8.html#quarto",
    "href": "Assignment8.html#quarto",
    "title": "Assignment 8: Census & Spatial Data",
    "section": "",
    "text": "Quarto enables you\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Assignment3.html",
    "href": "Assignment3.html",
    "title": "Assignment 3: Quanteda",
    "section": "",
    "text": "Biden-Xi Summit\nThe Biden-Xi summit was a virtual summit between U.S. President Joe Biden and Chinese President Xi Jinping, and was held on November 15 and 16, 2021. This summit was a critical diplomatic meeting between the leaders of the United States and China and aimed to address a range of important issues in the U.S.-China relationship, such as bilateral relations, economic and trade issues, human rights and climate change, regional and cyber security, and global issues such as COVID 19.\nWe used the provided R code and the Quanteda package to perform a text analysis on Twitter data regarding the Biden-Xi Summit. The following is what was identified from the analysis:\nThis line of code (head(tweet_dfm)) helps provide insight of the document terms in the data matrix.\n\n\n\n\n\nThis line of code (head(toptag, 10)) extracts the top 10 frequently used hashtags in the data matrix. As you can see, these hashtags are generically focused on the countries and presidents that the summit revolves around, but also touches on some global issues, such as COVID-19, drug usage, and the Uyghur Genocide.\n\n\n\n\n\nThese lines of code (head(tag_fcm)) explores the relationship between the hashtags, and how they are used together. This image shows the analysis on the first few rows of hashtags, and how they correspond together.\n\n\n\n\n\nThis network plot shows visually the relationship between the hashtags, and how they correspond with each other.\n\n\n\n\n\nThis line of code (head(topuser, 20)) extracts the top 20 frequent Twitter usernames found in the data matrix.\n\n\n\n\n\nThis image shows a network plot of a subset of Twitter usernames that were pulled from the data matrix, to show the visual relationship between usernames and their frequency of co-occurence in the matrix.\n\n\n\n\n\n\n\nU.S. Presidential Inaugural Speeches\nTo analyze Presidential Inaugural Speeches, I used the Quanteda package in R Studio to generate visualizations. The following is what I identified from my analysis:\n\n\n\n\n\nTo retrieve this image, we used the code ’kwic(tokens(data_corpus_inaugural_subset), pattern = “american”) %&gt;%\ntextplot_xray().\nBy using this code, we perform a keyword-in-context analysis and specifically search for the word “american” in the data matrix. It then generates the visualization so we can see the frequency in which the term “american” was used in each president’s inaugural speeches.\n\n\n\n\n\nWe use a similar code to derive this visualization, except we add in the functions to generate for the keywords “people” and “communist” as well. This visualization shows us how frequently each word was used in each president’s inaugural addresses, and how they compare to each of the other words as well.\nOver time and between presidents, the usage of the term “american” varied, but the usage of the term itself was not that high. The usage of the term “people” was a lot higher, and was used frequently in every president’s inaugural address.\n\n\nWhat is Wordfish?\nWordfish is a Quanteda Package function used in R Studio to analyze the relationship between words in documents. It is often used for text scaling, and will show the associated strength of each word within a document. To use this function, use ‘textstat_wordfish()’.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Assignment7.html",
    "href": "Assignment7.html",
    "title": "Assignment 7: Gov. Data & Parallel Processing",
    "section": "",
    "text": "Gov. Data & Parallel Processing\nWith the R Scripts in govdata01.r and parallel01.r, we are attempting to download data sets from government info and clean the variables, exporting them out into individual data sets in PDF format. Unfortunately, however, my script repeatedly failed to download the PDF’s of the government info data, as R kept informing me it could not read the table due to there being more columns than column names.\n\n\n\n\n\nHowever, I attempted the given exercise, which was to process data for “118th Congress Congressional Hearings in Committee on Foreign Affairs?” but a similar problem persisted. Due to this, I was unable to run a parallel computation using the parallel01.r script.\n\nStorage Planning\nTools like “tic tac” are crucial when dealing with mass downloads of data, as this tool helps scale data to prevent the overwhelming responses that comes with the downloading. Effective storage planning is important to see success in your code, as otherwise the download could occupy your entire hard drive. It’s also important to keep files organized within your hard drive to ensure easy access to locate the downloaded files.\n\n\nAdditional Storage Methods\nArrow: Arrow is a columnar, in-memory data format that facilitates fast analytics on large datasets. In R, the Arrow package provides an interface for reading and writing Arrow data. With efficient memory utilization and support for various data types, Arrow enables seamless data interchange between different systems, making it an excellent choice for high-performance computing tasks.\nFeather: Feather is a binary columnar data format designed for optimal performance and compatibility. In R, the feather package allows users to read and write data in the Feather format. Feather excels in terms of speed and efficiency, making it suitable for scenarios where quick and reliable data exchange is crucial. Its simplicity and cross-language support make it an attractive option for data scientists working on diverse projects.\nParquet: Parquet is a columnar storage file format that is highly optimized for use with big data processing frameworks. In R, the parquet package facilitates the reading and writing of data in the Parquet format. Parquet is known for its compression capabilities and schema evolution support, making it well-suited for data-intensive applications. Its compatibility with various programming languages and big data tools enhances its versatility in large-scale data processing workflows.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Assignment2.html",
    "href": "Assignment2.html",
    "title": "Assignment 2: Google Trends Data",
    "section": "",
    "text": "I used Google Trends and the gtrendsR Package to analyze data regarding the terms “Trump, Biden and Election”.\nWhen analyzing these terms on Google Trends, I noticed that the term “election” peaked on November 6, 2022, while the terms “Trump” and “Biden” stayed relatively stable all throughout 2022 and 2023. The term “Trump” had several hits around April and August of 2023, but it is safe to infer that it was not in correlation with the term “election”, as this was likely due to Trump’s presence in court cases at the time.\n\n\n\n\n\nUsing the gtrends package in R Studio yielded different results. The graph displayed by R studio shows that “Trump” was a popular term searched since 2005, but the term started picking up in search frequency around the 2016 election time. His next peak was in 2023, again likely due to his ongoing court cases. The term “election”, however was the most consistent in its peak, spiking every 4 years and then dying back down. It hit its all time peak this year in 2023, however, reaching a search frequency of 100 search hits. The term “Biden” had the lowest search hits, and had very minimal spikes in his trends.\n\n\n\n\n\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Assignment1.html",
    "href": "Assignment1.html",
    "title": "Assignment 1: Qualtrics Survey",
    "section": "",
    "text": "Link to survey: https://utdallas.qualtrics.com/jfe/form/SV_8qYDz8fe1fd8nZQ\nSurvey Analysis:\n\nThe survey is structured linearly to collect data and gauge responses on how frequently the sample population watch movies, and what content in movies they prefer. This is asked before addressing the demographics of the sample population.\nThe questionnaire is composed of multiple choice questions (objective questions), as well as Likert Scales questions. This is used to collect data based on the opinions of the sample population.\nThe questions are ordered by starting with a Likert Scales question, that then continues into multiple choice questions based on the opinions of the topic. The questionnaire concludes by collecting information on the sample population’s demographics.\n\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "CSV.html",
    "href": "CSV.html",
    "title": "My Resume",
    "section": "",
    "text": "469.275.3300; jhanvi_kannan@yahoo.com\nhttp://www.linkedin.com/in/jhanvikannan\n\n\n\nThe University of Texas at Dallas\nM.S.,Social Data Analytics and Research — August 2023 – December 2024\nB.S., Business Administration in Innovation and Entrepreneurship — Graduated May 2023 \nAcademic Excellence Scholarship (AES) Recipient\n\n\n\nGEICO- Richardson, Texas — June 6, 2022 – July 29, 2022\nSummer Business Leadership Intern\n· Conducted in-depth research on insurance trends and collected renewal data for underwriting submissions.\n· Collaborated with peers and completed a project to improve work culture in a post-pandemic environment.\n· Collaborated with peers and completed a project on the implementation of an AI chat system to enhance customer interactions.\n· Created training modules for employees to ensure a seamless transition to the new chat system, contributing to improved customer experiences.\n· Participated in research and training of insurance policies within GEICO’s regulations.\n· Maintained client confidentiality and ensured data accuracy.\nEno’s Pizza Tavern- Cypress Waters, Texas — May 2019 – August 2019, June 2020 – October 2021\nShift Manager\n· Oversaw restaurant operations, including opening, closing, and front/back-of-house management and training.\n· Conducted comprehensive training for FOH staff, ensuring proficiency in food and beverage knowledge.\n· Managed payroll, nightly and weekly audits, and sales reports.\n· Implemented weekly audits to forecast income accurately.\n\n\n\n· Proficient in Photoshop, Microsoft Software, Salesforce, SQL, Tableau, R, and Python.\n· Business & Public Law, Professional Development, Financial Accounting, Principles of Marketing, Management Methods of Decision Making, Management Accounting, Operations Management, Business Finance, Digital Prospecting, Methods of Data Collection, Research Design, Business Statistics.\n\n\n\n· Effective communication, Teamwork, Problem-Solving, Social Media Marketing, Leadership, Fundraising\n· Eligibility: (USPR) Eligible to work in the U.S. with no restrictions\n\n\n\nAwaazein, South Asian A Cappella Competition || Executive Director\n· Coordinated a National A Cappella Competition, managing a budget of $18,000 and achieving a profit.\n· Developed a virtual voting system for team selection to compete.\n· Secured sponsorships from over 50 local vendors and business.\nAkshaya Patra, Non-Profit Organization || President\n· Guided a non-profit organization that helps feed impoverished kids in India, while helping fund their education.\n· Raised $5,700 in one academic year to provide for impoverished children, while also bringing Indian cultural activities to UTD Student life.\n· Implemented Akshaya Patra as an official NGO Partner for a national A Cappella competition and a national classical dance competition.\n· Planned and executed monthly fundraising activities with 100+ active members.\nHansini, Classical Dance Competition || Marketing/PR Chair\n· Promoted UTD’s classical dance competition through social media and outreach.\n· Utilized Photoshop and Canva for content creation.\nAaja Nachle, South Asian Dance Competition || Registration & Finance Chair\n· Implemented an online anonymous voting system for team selection to compete.\n· Managed logistics and funding for the 8 competing teams, and organized fundraising efforts at UTD.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "CSV.html#quarto",
    "href": "CSV.html#quarto",
    "title": "csv",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "CSV.html#running-code",
    "href": "CSV.html#running-code",
    "title": "csv",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "CSV.html#jhanvi-kannan",
    "href": "CSV.html#jhanvi-kannan",
    "title": "My Resume",
    "section": "",
    "text": "469.275.3300; jhanvi_kannan@yahoo.com\nhttp://www.linkedin.com/in/jhanvikannan\n\n\n\nThe University of Texas at Dallas\nM.S.,Social Data Analytics and Research — August 2023 – December 2024\nB.S., Business Administration in Innovation and Entrepreneurship — Graduated May 2023 \nAcademic Excellence Scholarship (AES) Recipient\n\n\n\nGEICO- Richardson, Texas — June 6, 2022 – July 29, 2022\nSummer Business Leadership Intern\n· Conducted in-depth research on insurance trends and collected renewal data for underwriting submissions.\n· Collaborated with peers and completed a project to improve work culture in a post-pandemic environment.\n· Collaborated with peers and completed a project on the implementation of an AI chat system to enhance customer interactions.\n· Created training modules for employees to ensure a seamless transition to the new chat system, contributing to improved customer experiences.\n· Participated in research and training of insurance policies within GEICO’s regulations.\n· Maintained client confidentiality and ensured data accuracy.\nEno’s Pizza Tavern- Cypress Waters, Texas — May 2019 – August 2019, June 2020 – October 2021\nShift Manager\n· Oversaw restaurant operations, including opening, closing, and front/back-of-house management and training.\n· Conducted comprehensive training for FOH staff, ensuring proficiency in food and beverage knowledge.\n· Managed payroll, nightly and weekly audits, and sales reports.\n· Implemented weekly audits to forecast income accurately.\n\n\n\n· Proficient in Photoshop, Microsoft Software, Salesforce, SQL, Tableau, R, and Python.\n· Business & Public Law, Professional Development, Financial Accounting, Principles of Marketing, Management Methods of Decision Making, Management Accounting, Operations Management, Business Finance, Digital Prospecting, Methods of Data Collection, Research Design, Business Statistics.\n\n\n\n· Effective communication, Teamwork, Problem-Solving, Social Media Marketing, Leadership, Fundraising\n· Eligibility: (USPR) Eligible to work in the U.S. with no restrictions\n\n\n\nAwaazein, South Asian A Cappella Competition || Executive Director\n· Coordinated a National A Cappella Competition, managing a budget of $18,000 and achieving a profit.\n· Developed a virtual voting system for team selection to compete.\n· Secured sponsorships from over 50 local vendors and business.\nAkshaya Patra, Non-Profit Organization || President\n· Guided a non-profit organization that helps feed impoverished kids in India, while helping fund their education.\n· Raised $5,700 in one academic year to provide for impoverished children, while also bringing Indian cultural activities to UTD Student life.\n· Implemented Akshaya Patra as an official NGO Partner for a national A Cappella competition and a national classical dance competition.\n· Planned and executed monthly fundraising activities with 100+ active members.\nHansini, Classical Dance Competition || Marketing/PR Chair\n· Promoted UTD’s classical dance competition through social media and outreach.\n· Utilized Photoshop and Canva for content creation.\nAaja Nachle, South Asian Dance Competition || Registration & Finance Chair\n· Implemented an online anonymous voting system for team selection to compete.\n· Managed logistics and funding for the 8 competing teams, and organized fundraising efforts at UTD.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "epps6313.html",
    "href": "epps6313.html",
    "title": "EPPS6313: Research Paper",
    "section": "",
    "text": "Below is a research paper that I collaborated on with a team for my Introduction to Quantitative Methods class. This research delves into the correlation between access to mental health providers across the United States and its impact on rates of teen pregnancy, high school graduation, and juvenile arrests."
  },
  {
    "objectID": "epps6302.html",
    "href": "epps6302.html",
    "title": "EPPS 6302: Research Paper",
    "section": "",
    "text": "This is our final project for Methods of Data Collection and Production. Our study delves into the repercussions of climate change on the well-being of Dallas-Fort Worth (DFW) residents and examines the adaptive measures adopted by the community to navigate the persistent challenges in the foreseeable future.\nClick here to view our Final Project Slides"
  },
  {
    "objectID": "Assignment8.html#spatialdata01.r",
    "href": "Assignment8.html#spatialdata01.r",
    "title": "Assignment 8: Census & Spatial Data",
    "section": "",
    "text": "In the spatialdata01.r code, we explore the median age by state in 2019 using an API Census key.\nThe code loads American Community Survey (ACS) 2019 variables using the “tidycensus::load_variables” function. It also loads ACS 2019 profile variables. The “get_acs” function retrieves ACS data for median age by state in 2019. The data is obtained at the state level, and the resulting graph is shown below.\n\n\n\n\n\nThe final map visualizes the median age by state in 2019, providing a geographical representation of the data. Each state is shaded according to its median age, with the darker colors indicating a higher median age and the lighter colors indicating a lower median age.\n\n\nThe given exercise in the spatialdata01.r file was to test the code to download 2009 and 2020 data as well and compare them. Rewriting the code to download 2009 data was simple, and the visual is provided below. However, downloading the 2020 data was not possible, as an error occurred reading “Error in get_acs():! The regular 1-year ACS for 2020 was not released and is not available in tidycensus.”\n\n\n\n\n\nAt first glances, there did not seem to be many differences between the 2009 and 2019 data, but there are definitely some changes present. For example, the median age in Montana has decreased over the decade, as well as in Minnesota, Pennsylvania, North Dakota and New Jersey. Realistically, the only place we see the median age increasing in is in the U.S. Territory."
  },
  {
    "objectID": "Assignment8.html#spatialdata02.r",
    "href": "Assignment8.html#spatialdata02.r",
    "title": "Assignment 8: Census & Spatial Data",
    "section": "spatialdata02.r",
    "text": "spatialdata02.r\nNext, we run the spatialdata02.r code. This code uses the “get_acs” function to retrieve income data for all tracts in Texas (state code “TX”) for the year 2020. The income data is then plotted and the result is a visualization of income estimates at the tract level for both Texas and Dallas County in the year 2020, providing insights into income distribution across geographical areas. The “tmap” and “mapview” functions enable users to create thematic maps and interactively explore the data.\n  \n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "6323HW2.html",
    "href": "6323HW2.html",
    "title": "Brush Up R and Quarto",
    "section": "",
    "text": "x &lt;- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)\n\n\n\n\n\nlength(x)  # What does length() do?\n\n[1] 3\n\nlength(y)\n\n[1] 3\n\n\n\n\n\n\nx+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"has_annotations\" \"x\"               \"y\"              \n\nrm(x,y) # Remove objects\nls()\n\n[1] \"has_annotations\"\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!\n\n\n\n\n\n?matrix\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9950618\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)\n\n\n\n\n\nmean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768\n\n\n\n\n\n\nx=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\nquartz_off_screen \n                2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "6323HW2.html#create-object-using-the-assignment-operator--",
    "href": "6323HW2.html#create-object-using-the-assignment-operator--",
    "title": "Brush Up R and Quarto",
    "section": "",
    "text": "x &lt;- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)"
  },
  {
    "objectID": "6323HW2.html#using-function",
    "href": "6323HW2.html#using-function",
    "title": "Brush Up R and Quarto",
    "section": "",
    "text": "length(x)  # What does length() do?\n\n[1] 3\n\nlength(y)\n\n[1] 3"
  },
  {
    "objectID": "6323HW2.html#using---operators",
    "href": "6323HW2.html#using---operators",
    "title": "Brush Up R and Quarto",
    "section": "",
    "text": "x+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"has_annotations\" \"x\"               \"y\"              \n\nrm(x,y) # Remove objects\nls()\n\n[1] \"has_annotations\"\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!"
  },
  {
    "objectID": "6323HW2.html#matrix-operations",
    "href": "6323HW2.html#matrix-operations",
    "title": "Brush Up R and Quarto",
    "section": "",
    "text": "?matrix\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9950618\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)"
  },
  {
    "objectID": "6323HW2.html#simple-descriptive-statistics-base",
    "href": "6323HW2.html#simple-descriptive-statistics-base",
    "title": "Brush Up R and Quarto",
    "section": "",
    "text": "mean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768"
  },
  {
    "objectID": "6323HW2.html#visualization-using-r-graphics-without-packages",
    "href": "6323HW2.html#visualization-using-r-graphics-without-packages",
    "title": "Brush Up R and Quarto",
    "section": "",
    "text": "x=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\nquartz_off_screen \n                2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "6323HW2.html#indexing-data-using",
    "href": "6323HW2.html#indexing-data-using",
    "title": "Brush Up R and Quarto",
    "section": "Indexing Data using []",
    "text": "Indexing Data using []\n\nA=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),] # What does -c() do?\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A) # Dimensions\n\n[1] 4 4"
  },
  {
    "objectID": "6323HW2.html#loading-data-from-github-remote",
    "href": "6323HW2.html#loading-data-from-github-remote",
    "title": "Brush Up R and Quarto",
    "section": "Loading Data from GitHub (remote)",
    "text": "Loading Data from GitHub (remote)\n\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\",header=T,na.strings=\"?\") \nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\") # read csv file\n# Which function reads data faster?\n\n# Try using this simple method\n# time1 = proc.time()\n# Auto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\",header=T,na.strings=\"?\")\n# proc.time()-time1\n\n# Check on data\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,] # select rows\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto) # Notice the difference?\n\n[1] 397   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\""
  },
  {
    "objectID": "6323HW2.html#load-data-from-islr-website",
    "href": "6323HW2.html#load-data-from-islr-website",
    "title": "Brush Up R and Quarto",
    "section": "Load data from ISLR website",
    "text": "Load data from ISLR website\n\nAuto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9"
  },
  {
    "objectID": "6323HW2.html#additional-graphical-and-numerical-summaries",
    "href": "6323HW2.html#additional-graphical-and-numerical-summaries",
    "title": "Brush Up R and Quarto",
    "section": "Additional Graphical and Numerical Summaries",
    "text": "Additional Graphical and Numerical Summaries\n\n# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\nhist(mpg)\n\n\n\nhist(mpg,col=2)\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\nplot(horsepower,mpg)\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60"
  },
  {
    "objectID": "6323HW2.html#linear-regression",
    "href": "6323HW2.html#linear-regression",
    "title": "Brush Up R and Quarto",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\n\nThe downloaded binary packages are in\n    /var/folders/n8/p8ct1r_92t92z2qhc173_sqr0000gn/T//RtmplHNQ9l/downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nLoading required package: ISLR\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375"
  },
  {
    "objectID": "6323HW2.html#multiple-linear-regression",
    "href": "6323HW2.html#multiple-linear-regression",
    "title": "Brush Up R and Quarto",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\nlm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)"
  },
  {
    "objectID": "6323HW2.html#non-linear-transformations-of-the-predictors",
    "href": "6323HW2.html#non-linear-transformations-of-the-predictors",
    "title": "Brush Up R and Quarto",
    "section": "Non-linear Transformations of the Predictors",
    "text": "Non-linear Transformations of the Predictors\n\nlm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "6323HW2.html#qualitative-predictors",
    "href": "6323HW2.html#qualitative-predictors",
    "title": "Brush Up R and Quarto",
    "section": "Qualitative Predictors",
    "text": "Qualitative Predictors\n\n# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1"
  },
  {
    "objectID": "6323HW2.html#interaction-terms-including-interaction-and-single-effects",
    "href": "6323HW2.html#interaction-terms-including-interaction-and-single-effects",
    "title": "Brush Up R and Quarto",
    "section": "Interaction Terms (including interaction and single effects)",
    "text": "Interaction Terms (including interaction and single effects)\n\nsummary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "6323HW2.html#problems-and-missing-values",
    "href": "6323HW2.html#problems-and-missing-values",
    "title": "Brush Up R and Quarto",
    "section": "Problems and Missing Values",
    "text": "Problems and Missing Values\nSome problems present in the dataset is that it is difficult to read. In Stata, we are able to use the code “codebook” to look at what the variable labels represent, but in R, it is difficult to do so. We see 1,690 entries, but are unable to identify what each value represents.\nTo identify missingness in large datasets, we can use visual methods to identify any gaps. Then we can exclude observations with missing values entirely from the analysis, if the missing values are not too prevalent and removing them does not significantly impact the analysis."
  },
  {
    "objectID": "6323HW2.html#tondu-and-other-variables",
    "href": "6323HW2.html#tondu-and-other-variables",
    "title": "Brush Up R and Quarto",
    "section": "Tondu and other Variables",
    "text": "Tondu and other Variables\nTo explore the relationship between the “Tondu” variable and other variables including “female”, “DPP”, “age”, “income”, “edu”, “Taiwanese”, and “Econ_worse”, you can use various statistical methods such as:\n\nDescriptive statistics: Calculate summary statistics (mean, median, standard deviation, etc.) for each variable and examine patterns.\n\n\n\nVisualization: Create visualizations such as scatter plots, histograms, box plots, or heatmaps to visualize the relationships between variables.\nCorrelation analysis: Compute correlation coefficients (e.g., Pearson correlation, Spearman correlation) to quantify the strength and direction of the relationships between continuous variables."
  },
  {
    "objectID": "6323HW2.html#the-votetsai-variable",
    "href": "6323HW2.html#the-votetsai-variable",
    "title": "Brush Up R and Quarto",
    "section": "The “votetsai” Variable",
    "text": "The “votetsai” Variable\nTo explore the “votetsai” variable (vote for DPP candidate Tsai Ing-wen), you can use similar methods as mentioned above:\n\nDescriptive statistics: Examine the distribution of votes for Tsai Ing-wen and calculate proportions or percentages.\nVisualization: Create bar plots or pie charts to visualize the distribution of votes for Tsai Ing-wen among different groups.\nCross-tabulation: Cross-tabulate “votetsai” with other relevant variables to explore any associations or patterns.\nStatistical tests: Conduct statistical tests (e.g., chi-square test) to examine whether there are significant differences in voting behavior across different demographic or socioeconomic groups."
  },
  {
    "objectID": "6323HW2.html#tondu-variable-frequency-and-bar-chart",
    "href": "6323HW2.html#tondu-variable-frequency-and-bar-chart",
    "title": "Brush Up R and Quarto",
    "section": "“Tondu” Variable Frequency and Bar Chart",
    "text": "“Tondu” Variable Frequency and Bar Chart\n\nTEDS_2016$Tondu &lt;- factor(TEDS_2016$Tondu,\n                           labels = c(\"Unification now\", \"Status quo, unif. in future\", \n                                      \"Status quo, decide later\", \"Status quo forever\", \n                                      \"Status quo, indep. in future\", \"Independence now\", \n                                      \"No response\"))\n\nfrequency_table &lt;- table(TEDS_2016$Tondu)\nprint(frequency_table)\n\n\n             Unification now  Status quo, unif. in future \n                          27                          180 \n    Status quo, decide later           Status quo forever \n                         546                          328 \nStatus quo, indep. in future             Independence now \n                         380                          108 \n                 No response \n                         121 \n\nbarplot(frequency_table, \n        main = \"Frequency of Tondu Responses\",\n        xlab = \"Tondu Responses\",\n        ylab = \"Frequency\",\n        col = \"skyblue\",\n        ylim = c(0, max(frequency_table) + 100),  # Adjust ylim for better visualization\n        las = 2)  # Rotate x-axis labels for better readability"
  }
]