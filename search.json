[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "M.S. Student in Social Data Analytics & Research at the University of Texas at Dallas.",
    "section": "",
    "text": "Hello! My name is Jhanvi Kannan, and I graduated from the University of Texas at Dallas in May 2023 with my Bachelor’s in Science in Business Administration, with a concentration in Innovation & Entrepreneurship. During my undergrad degree, I focused on many different elements of being an entrepreneur, such as sales and marketing, as well as testing the viability of new products and the estimated demand for it in a market.\nWith my masters degree, I am hoping to fine tune my research skills and use data analysis to help improve my abilities to be a great entrepreneur. Using this masters, I would be well equipped to research the demand of new technology needed in a community, and help bring that technology forth. I aspire to help push the world to grow in a well-mannered and ethical way by focusing on what people need to make their lives better in a day-to-day environment.\nClick here to view our Final Project\nContact Me: jhanvi.kannan@utdallas.edu\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Assignment 1: Qualtrics Survey",
    "section": "",
    "text": "Link to survey: https://utdallas.qualtrics.com/jfe/form/SV_8qYDz8fe1fd8nZQ\nSurvey Analysis:\n\nThe survey is structured linearly to collect data and gauge responses on how frequently the sample population watch movies, and what content in movies they prefer. This is asked before addressing the demographics of the sample population.\nThe questionnaire is composed of multiple choice questions (objective questions), as well as Likert Scales questions. This is used to collect data based on the opinions of the sample population.\nThe questions are ordered by starting with a Likert Scales question, that then continues into multiple choice questions based on the opinions of the topic. The questionnaire concludes by collecting information on the sample population’s demographics.\n\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#jhanvi.kannanutdallas.edu",
    "href": "index.html#jhanvi.kannanutdallas.edu",
    "title": "jhanvi-kannan.github.io",
    "section": "jhanvi.kannan@utdallas.edu",
    "text": "jhanvi.kannan@utdallas.edu"
  },
  {
    "objectID": "index.html#assignment-1-qualtrics-survey",
    "href": "index.html#assignment-1-qualtrics-survey",
    "title": "jhanvi-kannan.github.io",
    "section": "Assignment 1: Qualtrics Survey",
    "text": "Assignment 1: Qualtrics Survey\nLink to survey: https://utdallas.qualtrics.com/jfe/form/SV_8qYDz8fe1fd8nZQ\nSurvey Analysis:\n\nThe survey is structured linearly to collect data and gauge responses on how frequently the sample population watch movies, and what content in movies they prefer. This is asked before addressing the demographics of the sample population.\nThe questionnaire is composed of multiple choice questions (objective questions), as well as Likert Scales questions. This is used to collect data based on the opinions of the sample population.\nThe questions are ordered by starting with a Likert Scales question, that then continues into multiple choice questions based on the opinions of the topic. The questionnaire concludes by collecting information on the sample population’s demographics.\n\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html#assignment-1-qualtrics-survey",
    "href": "about.html#assignment-1-qualtrics-survey",
    "title": "Assignment 1: Qualtrics Survey",
    "section": "",
    "text": "Link to survey: https://utdallas.qualtrics.com/jfe/form/SV_8qYDz8fe1fd8nZQ\nSurvey Analysis:\n\nThe survey is structured linearly to collect data and gauge responses on how frequently the sample population watch movies, and what content in movies they prefer. This is asked before addressing the demographics of the sample population.\nThe questionnaire is composed of multiple choice questions (objective questions), as well as Likert Scales questions. This is used to collect data based on the opinions of the sample population.\nThe questions are ordered by starting with a Likert Scales question, that then continues into multiple choice questions based on the opinions of the topic. The questionnaire concludes by collecting information on the sample population’s demographics.\n\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Assignment 2.html",
    "href": "Assignment 2.html",
    "title": "Assignment 2: Google Trends Data",
    "section": "",
    "text": "I used Google Trends and the gtrendsR Package to analyze data regarding the terms “Trump, Biden and Election”.\nWhen analyzing these terms on Google Trends, I noticed that the term “election” peaked on November 6, 2022, while the terms “Trump” and “Biden” stayed relatively stable all throughout 2022 and 2023. The term “Trump” had several hits around April and August of 2023, but it is safe to infer that it was not in correlation with the term “election”, as this was likely due to Trump’s presence in court cases at the time.\n\n\n\n\n\nUsing the gtrends package in R Studio yielded different results. The graph displayed by R studio shows that “Trump” was a popular term searched since 2005, but the term started picking up in search frequency around the 2016 election time. His next peak was in 2023, again likely due to his ongoing court cases. The term “election”, however was the most consistent in its peak, spiking every 4 years and then dying back down. It hit its all time peak this year in 2023, however, reaching a search frequency of 100 search hits. The term “Biden” had the lowest search hits, and had very minimal spikes in his trends.\n\n\n\n\n\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Assignment 3.html",
    "href": "Assignment 3.html",
    "title": "Assignment 3: Quanteda",
    "section": "",
    "text": "Biden-Xi Summit\nThe Biden-Xi summit was a virtual summit between U.S. President Joe Biden and Chinese President Xi Jinping, and was held on November 15 and 16, 2021. This summit was a critical diplomatic meeting between the leaders of the United States and China and aimed to address a range of important issues in the U.S.-China relationship, such as bilateral relations, economic and trade issues, human rights and climate change, regional and cyber security, and global issues such as COVID 19.\nWe used the provided R code and the Quanteda package to perform a text analysis on Twitter data regarding the Biden-Xi Summit. The following is what was identified from the analysis:\nThis line of code (head(tweet_dfm)) helps provide insight of the document terms in the data matrix.\n\n\n\n\n\nThis line of code (head(toptag, 10)) extracts the top 10 frequently used hashtags in the data matrix. As you can see, these hashtags are generically focused on the countries and presidents that the summit revolves around, but also touches on some global issues, such as COVID-19, drug usage, and the Uyghur Genocide.\n\n\n\n\n\nThese lines of code (head(tag_fcm)) explores the relationship between the hashtags, and how they are used together. This image shows the analysis on the first few rows of hashtags, and how they correspond together.\n\n\n\n\n\nThis network plot shows visually the relationship between the hashtags, and how they correspond with each other.\n\n\n\n\n\nThis line of code (head(topuser, 20)) extracts the top 20 frequent Twitter usernames found in the data matrix.\n\n\n\n\n\nThis image shows a network plot of a subset of Twitter usernames that were pulled from the data matrix, to show the visual relationship between usernames and their frequency of co-occurence in the matrix.\n\n\n\n\n\n\n\nU.S. Presidential Inaugural Speeches\nTo analyze Presidential Inaugural Speeches, I used the Quanteda package in R Studio to generate visualizations. The following is what I identified from my analysis:\n\n\n\n\n\nTo retrieve this image, we used the code ’kwic(tokens(data_corpus_inaugural_subset), pattern = “american”) %&gt;%\ntextplot_xray().\nBy using this code, we perform a keyword-in-context analysis and specifically search for the word “american” in the data matrix. It then generates the visualization so we can see the frequency in which the term “american” was used in each president’s inaugural speeches.\n\n\n\n\n\nWe use a similar code to derive this visualization, except we add in the functions to generate for the keywords “people” and “communist” as well. This visualization shows us how frequently each word was used in each president’s inaugural addresses, and how they compare to each of the other words as well.\nOver time and between presidents, the usage of the term “american” varied, but the usage of the term itself was not that high. The usage of the term “people” was a lot higher, and was used frequently in every president’s inaugural address.\n\n\nWhat is Wordfish?\nWordfish is a Quanteda Package function used in R Studio to analyze the relationship between words in documents. It is often used for text scaling, and will show the associated strength of each word within a document. To use this function, use ‘textstat_wordfish()’.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Assignment 6.html",
    "href": "Assignment 6.html",
    "title": "Assignment 6: Webscraping",
    "section": "",
    "text": "The textmining01.r code is a script for performing text mining and generating a word cloud from downloaded text data regarding Martin Luther King Jr.’s “I have a Dream” speech. The script downloads data using the “htmlTreeParse” function and stores it as a variable. The text is then pre-processed and vectorized, with the text being converted to all lower case, number and punctuation stripped away, and a matrix formed to represent the frequencies. The matrix is shown below:\n\n\n\n\n\nThe “wordcounts” variable is then used to order these frequencies in descending order, shown below:\n\n\n\n\n\nWith this, we can then formulate Word Clouds with our processed text data. The larger the word is presented in the Word Cloud, the more frequent it is presented in the speech.\n \n\n\nNext, we will conduct a similar analysis on Winston Churchill’s speech “The Finest Hour”. We will use the following full script to analyze the text.\n\n\nThe Word Cloud is presented below:\n\n\n\n\n\nHere we can see from our text analysis that words such as “war”, “france” and “french”, and “great” are very frequent in the speech."
  },
  {
    "objectID": "Assignment 6.html#their-finest-hour--winston-churchill",
    "href": "Assignment 6.html#their-finest-hour--winston-churchill",
    "title": "Assignment 6: Webscraping",
    "section": "",
    "text": "Next, we will conduct a similar analysis on Winston Churchill’s speech “The Finest Hour”. We will use the following full script to analyze the text.\n\n\nThe Word Cloud is presented below:\n\n\n\n\n\nHere we can see from our text analysis that words such as “war”, “france” and “french”, and “great” are very frequent in the speech."
  },
  {
    "objectID": "Assignment 7.html",
    "href": "Assignment 7.html",
    "title": "Assignment 7: Gov. Data & Parallel Processing",
    "section": "",
    "text": "With the R Scripts in govdata01.r and parallel01.r, we are attempting to download data sets from government info and clean the variables, exporting them out into individual data sets in PDF format. Unfortunately, however, my script repeatedly failed to download the PDFs of the government info data, as R kept informing me it could not read the table due to there being more columns than column names.\nHowever, I attempted the given exercise, which was to process data for \"118th Congress Congressional Hearings in Committee on Foreign Affairs?\" but a similar problem persisted. Due to this, I was unable to run a parallel computation using the parallel01.r script."
  },
  {
    "objectID": "Assignment6.html",
    "href": "Assignment6.html",
    "title": "Assignment 6: Webscraping",
    "section": "",
    "text": "The textmining01.r code is a script for performing text mining and generating a word cloud from downloaded text data regarding Martin Luther King Jr.’s “I have a Dream” speech. The script downloads data using the “htmlTreeParse” function and stores it as a variable. The text is then pre-processed and vectorized, with the text being converted to all lower case, number and punctuation stripped away, and a matrix formed to represent the frequencies. The matrix is shown below:\n\n\n\n\n\nThe “wordcounts” variable is then used to order these frequencies in descending order, shown below:\n\n\n\n\n\nWith this, we can then formulate Word Clouds with our processed text data. The larger the word is presented in the Word Cloud, the more frequent it is presented in the speech.\n \n\n\nNext, we will conduct a similar analysis on Winston Churchill’s speech “The Finest Hour”. We will use the following full script to analyze the text.\n\n\nThe Word Cloud is presented below:\n\n\n\n\n\nHere we can see from our text analysis that words such as “war”, “france” and “french”, and “great” are very frequent in the speech."
  },
  {
    "objectID": "Assignment6.html#their-finest-hour--winston-churchill",
    "href": "Assignment6.html#their-finest-hour--winston-churchill",
    "title": "Assignment 6: Webscraping",
    "section": "",
    "text": "Next, we will conduct a similar analysis on Winston Churchill’s speech “The Finest Hour”. We will use the following full script to analyze the text.\n\n\nThe Word Cloud is presented below:\n\n\n\n\n\nHere we can see from our text analysis that words such as “war”, “france” and “french”, and “great” are very frequent in the speech."
  },
  {
    "objectID": "Assignment8.html",
    "href": "Assignment8.html",
    "title": "Assignment 8: Census & Spatial Data",
    "section": "",
    "text": "Quarto enables you\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Assignment8.html#quarto",
    "href": "Assignment8.html#quarto",
    "title": "Assignment 8: Census & Spatial Data",
    "section": "",
    "text": "Quarto enables you\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Assignment3.html",
    "href": "Assignment3.html",
    "title": "Assignment 3: Quanteda",
    "section": "",
    "text": "Biden-Xi Summit\nThe Biden-Xi summit was a virtual summit between U.S. President Joe Biden and Chinese President Xi Jinping, and was held on November 15 and 16, 2021. This summit was a critical diplomatic meeting between the leaders of the United States and China and aimed to address a range of important issues in the U.S.-China relationship, such as bilateral relations, economic and trade issues, human rights and climate change, regional and cyber security, and global issues such as COVID 19.\nWe used the provided R code and the Quanteda package to perform a text analysis on Twitter data regarding the Biden-Xi Summit. The following is what was identified from the analysis:\nThis line of code (head(tweet_dfm)) helps provide insight of the document terms in the data matrix.\n\n\n\n\n\nThis line of code (head(toptag, 10)) extracts the top 10 frequently used hashtags in the data matrix. As you can see, these hashtags are generically focused on the countries and presidents that the summit revolves around, but also touches on some global issues, such as COVID-19, drug usage, and the Uyghur Genocide.\n\n\n\n\n\nThese lines of code (head(tag_fcm)) explores the relationship between the hashtags, and how they are used together. This image shows the analysis on the first few rows of hashtags, and how they correspond together.\n\n\n\n\n\nThis network plot shows visually the relationship between the hashtags, and how they correspond with each other.\n\n\n\n\n\nThis line of code (head(topuser, 20)) extracts the top 20 frequent Twitter usernames found in the data matrix.\n\n\n\n\n\nThis image shows a network plot of a subset of Twitter usernames that were pulled from the data matrix, to show the visual relationship between usernames and their frequency of co-occurence in the matrix.\n\n\n\n\n\n\n\nU.S. Presidential Inaugural Speeches\nTo analyze Presidential Inaugural Speeches, I used the Quanteda package in R Studio to generate visualizations. The following is what I identified from my analysis:\n\n\n\n\n\nTo retrieve this image, we used the code ’kwic(tokens(data_corpus_inaugural_subset), pattern = “american”) %&gt;%\ntextplot_xray().\nBy using this code, we perform a keyword-in-context analysis and specifically search for the word “american” in the data matrix. It then generates the visualization so we can see the frequency in which the term “american” was used in each president’s inaugural speeches.\n\n\n\n\n\nWe use a similar code to derive this visualization, except we add in the functions to generate for the keywords “people” and “communist” as well. This visualization shows us how frequently each word was used in each president’s inaugural addresses, and how they compare to each of the other words as well.\nOver time and between presidents, the usage of the term “american” varied, but the usage of the term itself was not that high. The usage of the term “people” was a lot higher, and was used frequently in every president’s inaugural address.\n\n\nWhat is Wordfish?\nWordfish is a Quanteda Package function used in R Studio to analyze the relationship between words in documents. It is often used for text scaling, and will show the associated strength of each word within a document. To use this function, use ‘textstat_wordfish()’.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Assignment7.html",
    "href": "Assignment7.html",
    "title": "Assignment 7: Gov. Data & Parallel Processing",
    "section": "",
    "text": "Gov. Data & Parallel Processing\nWith the R Scripts in govdata01.r and parallel01.r, we are attempting to download data sets from government info and clean the variables, exporting them out into individual data sets in PDF format. Unfortunately, however, my script repeatedly failed to download the PDF’s of the government info data, as R kept informing me it could not read the table due to there being more columns than column names.\n\n\n\n\n\nHowever, I attempted the given exercise, which was to process data for “118th Congress Congressional Hearings in Committee on Foreign Affairs?” but a similar problem persisted. Due to this, I was unable to run a parallel computation using the parallel01.r script.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Assignment2.html",
    "href": "Assignment2.html",
    "title": "Assignment 2: Google Trends Data",
    "section": "",
    "text": "I used Google Trends and the gtrendsR Package to analyze data regarding the terms “Trump, Biden and Election”.\nWhen analyzing these terms on Google Trends, I noticed that the term “election” peaked on November 6, 2022, while the terms “Trump” and “Biden” stayed relatively stable all throughout 2022 and 2023. The term “Trump” had several hits around April and August of 2023, but it is safe to infer that it was not in correlation with the term “election”, as this was likely due to Trump’s presence in court cases at the time.\n\n\n\n\n\nUsing the gtrends package in R Studio yielded different results. The graph displayed by R studio shows that “Trump” was a popular term searched since 2005, but the term started picking up in search frequency around the 2016 election time. His next peak was in 2023, again likely due to his ongoing court cases. The term “election”, however was the most consistent in its peak, spiking every 4 years and then dying back down. It hit its all time peak this year in 2023, however, reaching a search frequency of 100 search hits. The term “Biden” had the lowest search hits, and had very minimal spikes in his trends.\n\n\n\n\n\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Assignment1.html",
    "href": "Assignment1.html",
    "title": "Assignment 1: Qualtrics Survey",
    "section": "",
    "text": "Link to survey: https://utdallas.qualtrics.com/jfe/form/SV_8qYDz8fe1fd8nZQ\nSurvey Analysis:\n\nThe survey is structured linearly to collect data and gauge responses on how frequently the sample population watch movies, and what content in movies they prefer. This is asked before addressing the demographics of the sample population.\nThe questionnaire is composed of multiple choice questions (objective questions), as well as Likert Scales questions. This is used to collect data based on the opinions of the sample population.\nThe questions are ordered by starting with a Likert Scales question, that then continues into multiple choice questions based on the opinions of the topic. The questionnaire concludes by collecting information on the sample population’s demographics.\n\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "CSV.html",
    "href": "CSV.html",
    "title": "My Resume",
    "section": "",
    "text": "469.275.3300; jhanvi_kannan@yahoo.com\nhttp://www.linkedin.com/in/jhanvikannan\n\n\n\nThe University of Texas at Dallas\nM.S.,Social Data Analytics and Research — August 2023 – December 2024\nB.S., Business Administration in Innovation and Entrepreneurship — Graduated May 2023 \nAcademic Excellence Scholarship (AES) Recipient\n\n\n\nGEICO- Richardson, Texas — June 6, 2022 – July 29, 2022\nSummer Business Leadership Intern\n· Conducted in-depth research on insurance trends and collected renewal data for underwriting submissions.\n· Assisted in processing and managing liability and auto claims.\n· Maintained client confidentiality and ensured data accuracy.\n· Collaborated with peers and completed a project to improve work culture in a post-pandemic environment.\n· Participated in research and training of insurance policies within GEICO’s regulations.\n· Communicated company policies to clients and assisted with auto insurance claims.\nEno’s Pizza Tavern- Cypress Waters, Texas — May 2019 – August 2019, June 2020 – October 2021\nShift Manager\n· Oversaw restaurant operations, including opening, closing, and front/back-of-house management and training.\n· Conducted comprehensive training for FOH staff, ensuring proficiency in food and beverage knowledge.\n· Managed payroll, nightly and weekly audits, and sales reports.\n· Implemented weekly audits to forecast income accurately.\n\n\n\n· Proficient in Photoshop, Microsoft Software, Salesforce, SQL, Tableau, R, and Python.\n· Business & Public Law, Professional Development, Financial Accounting, Principles of Marketing, Management Methods of Decision Making, Management Accounting, Operations Management, Business Finance, Digital Prospecting, Methods of Data Collection, Research Design, Business Statistics.\n\n\n\n· Effective communication, Teamwork, Problem-Solving, Social Media Marketing, Leadership, Fundraising\n· Eligibility: (USPR) Eligible to work in the U.S. with no restrictions\n\n\n\nAwaazein, South Asian A Cappella Competition || Executive Director\n· Coordinated a National A Cappella Competition, managing a budget of $18,000 and achieving a profit.\n· Developed a virtual voting system for team selection to compete.\n· Secured sponsorships from over 50 local vendors and business.\nAkshaya Patra, Non-Profit Organization || President\n· Guided a non-profit organization that helps feed impoverished kids in India, while helping fund their education.\n· Raised $5,700 in one academic year to provide for impoverished children, while also bringing Indian cultural activities to UTD Student life.\n· Implemented Akshaya Patra as an official NGO Partner for a national A Cappella competition and a national classical dance competition.\n· Planned and executed monthly fundraising activities with 100+ active members.\nHansini, Classical Dance Competition || Marketing/PR Chair\n· Promoted UTD’s classical dance competition through social media and outreach.\n· Utilized Photoshop and Canva for content creation.\nAaja Nachle, South Asian Dance Competition || Registration & Finance Chair\n· Implemented an online anonymous voting system for team selection to compete.\n· Managed logistics and funding for the 8 competing teams, and organized fundraising efforts at UTD.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "CSV.html#quarto",
    "href": "CSV.html#quarto",
    "title": "csv",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "CSV.html#running-code",
    "href": "CSV.html#running-code",
    "title": "csv",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "CSV.html#jhanvi-kannan",
    "href": "CSV.html#jhanvi-kannan",
    "title": "My Resume",
    "section": "",
    "text": "469.275.3300; jhanvi_kannan@yahoo.com\nhttp://www.linkedin.com/in/jhanvikannan\n\n\n\nThe University of Texas at Dallas\nM.S.,Social Data Analytics and Research — August 2023 – December 2024\nB.S., Business Administration in Innovation and Entrepreneurship — Graduated May 2023 \nAcademic Excellence Scholarship (AES) Recipient\n\n\n\nGEICO- Richardson, Texas — June 6, 2022 – July 29, 2022\nSummer Business Leadership Intern\n· Conducted in-depth research on insurance trends and collected renewal data for underwriting submissions.\n· Assisted in processing and managing liability and auto claims.\n· Maintained client confidentiality and ensured data accuracy.\n· Collaborated with peers and completed a project to improve work culture in a post-pandemic environment.\n· Participated in research and training of insurance policies within GEICO’s regulations.\n· Communicated company policies to clients and assisted with auto insurance claims.\nEno’s Pizza Tavern- Cypress Waters, Texas — May 2019 – August 2019, June 2020 – October 2021\nShift Manager\n· Oversaw restaurant operations, including opening, closing, and front/back-of-house management and training.\n· Conducted comprehensive training for FOH staff, ensuring proficiency in food and beverage knowledge.\n· Managed payroll, nightly and weekly audits, and sales reports.\n· Implemented weekly audits to forecast income accurately.\n\n\n\n· Proficient in Photoshop, Microsoft Software, Salesforce, SQL, Tableau, R, and Python.\n· Business & Public Law, Professional Development, Financial Accounting, Principles of Marketing, Management Methods of Decision Making, Management Accounting, Operations Management, Business Finance, Digital Prospecting, Methods of Data Collection, Research Design, Business Statistics.\n\n\n\n· Effective communication, Teamwork, Problem-Solving, Social Media Marketing, Leadership, Fundraising\n· Eligibility: (USPR) Eligible to work in the U.S. with no restrictions\n\n\n\nAwaazein, South Asian A Cappella Competition || Executive Director\n· Coordinated a National A Cappella Competition, managing a budget of $18,000 and achieving a profit.\n· Developed a virtual voting system for team selection to compete.\n· Secured sponsorships from over 50 local vendors and business.\nAkshaya Patra, Non-Profit Organization || President\n· Guided a non-profit organization that helps feed impoverished kids in India, while helping fund their education.\n· Raised $5,700 in one academic year to provide for impoverished children, while also bringing Indian cultural activities to UTD Student life.\n· Implemented Akshaya Patra as an official NGO Partner for a national A Cappella competition and a national classical dance competition.\n· Planned and executed monthly fundraising activities with 100+ active members.\nHansini, Classical Dance Competition || Marketing/PR Chair\n· Promoted UTD’s classical dance competition through social media and outreach.\n· Utilized Photoshop and Canva for content creation.\nAaja Nachle, South Asian Dance Competition || Registration & Finance Chair\n· Implemented an online anonymous voting system for team selection to compete.\n· Managed logistics and funding for the 8 competing teams, and organized fundraising efforts at UTD.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "epps6313.html",
    "href": "epps6313.html",
    "title": "EPPS6313: Research Paper",
    "section": "",
    "text": "Below is a research paper that I collaborated on with a team for my Introduction to Quantitative Methods class. This research delves into the correlation between access to mental health providers across the United States and its impact on rates of teen pregnancy, high school graduation, and juvenile arrests."
  }
]