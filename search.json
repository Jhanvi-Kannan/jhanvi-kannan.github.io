[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "M.S. Student in Social Data Analytics & Research at the University of Texas at Dallas.",
    "section": "",
    "text": "Hello! My name is Jhanvi Kannan, and I graduated from the University of Texas at Dallas in May 2023 with my Bachelor’s in Science in Business Administration, with a concentration in Innovation & Entrepreneurship. During my undergrad degree, I focused on many different elements of being an entrepreneur, such as sales and marketing, as well as testing the viability of new products and the estimated demand for it in a market.\nWith my masters degree, I am hoping to fine tune my research skills and use data analysis to help improve my abilities to be a great entrepreneur. Using this masters, I would be well equipped to research the demand of new technology needed in a community, and help bring that technology forth. I aspire to help push the world to grow in a well-mannered and ethical way by focusing on what people need to make their lives better in a day-to-day environment.\nContact Me: jhanvi.kannan@utdallas.edu\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Assignment 1: Qualtrics Survey",
    "section": "",
    "text": "Link to survey: https://utdallas.qualtrics.com/jfe/form/SV_8qYDz8fe1fd8nZQ\nSurvey Analysis:\n\nThe survey is structured linearly to collect data and gauge responses on how frequently the sample population watch movies, and what content in movies they prefer. This is asked before addressing the demographics of the sample population.\nThe questionnaire is composed of multiple choice questions (objective questions), as well as Likert Scales questions. This is used to collect data based on the opinions of the sample population.\nThe questions are ordered by starting with a Likert Scales question, that then continues into multiple choice questions based on the opinions of the topic. The questionnaire concludes by collecting information on the sample population’s demographics.\n\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#jhanvi.kannanutdallas.edu",
    "href": "index.html#jhanvi.kannanutdallas.edu",
    "title": "jhanvi-kannan.github.io",
    "section": "jhanvi.kannan@utdallas.edu",
    "text": "jhanvi.kannan@utdallas.edu"
  },
  {
    "objectID": "index.html#assignment-1-qualtrics-survey",
    "href": "index.html#assignment-1-qualtrics-survey",
    "title": "jhanvi-kannan.github.io",
    "section": "Assignment 1: Qualtrics Survey",
    "text": "Assignment 1: Qualtrics Survey\nLink to survey: https://utdallas.qualtrics.com/jfe/form/SV_8qYDz8fe1fd8nZQ\nSurvey Analysis:\n\nThe survey is structured linearly to collect data and gauge responses on how frequently the sample population watch movies, and what content in movies they prefer. This is asked before addressing the demographics of the sample population.\nThe questionnaire is composed of multiple choice questions (objective questions), as well as Likert Scales questions. This is used to collect data based on the opinions of the sample population.\nThe questions are ordered by starting with a Likert Scales question, that then continues into multiple choice questions based on the opinions of the topic. The questionnaire concludes by collecting information on the sample population’s demographics.\n\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html#assignment-1-qualtrics-survey",
    "href": "about.html#assignment-1-qualtrics-survey",
    "title": "Assignment 1: Qualtrics Survey",
    "section": "",
    "text": "Link to survey: https://utdallas.qualtrics.com/jfe/form/SV_8qYDz8fe1fd8nZQ\nSurvey Analysis:\n\nThe survey is structured linearly to collect data and gauge responses on how frequently the sample population watch movies, and what content in movies they prefer. This is asked before addressing the demographics of the sample population.\nThe questionnaire is composed of multiple choice questions (objective questions), as well as Likert Scales questions. This is used to collect data based on the opinions of the sample population.\nThe questions are ordered by starting with a Likert Scales question, that then continues into multiple choice questions based on the opinions of the topic. The questionnaire concludes by collecting information on the sample population’s demographics.\n\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Assignment 2.html",
    "href": "Assignment 2.html",
    "title": "Assignment 2: Google Trends Data",
    "section": "",
    "text": "I used Google Trends and the gtrendsR Package to analyze data regarding the terms “Trump, Biden and Election”.\nWhen analyzing these terms on Google Trends, I noticed that the term “election” peaked on November 6, 2022, while the terms “Trump” and “Biden” stayed relatively stable all throughout 2022 and 2023. The term “Trump” had several hits around April and August of 2023, but it is safe to infer that it was not in correlation with the term “election”, as this was likely due to Trump’s presence in court cases at the time.\n\n\n\n\n\nUsing the gtrends package in R Studio yielded different results. The graph displayed by R studio shows that “Trump” was a popular term searched since 2005, but the term started picking up in search frequency around the 2016 election time. His next peak was in 2023, again likely due to his ongoing court cases. The term “election”, however was the most consistent in its peak, spiking every 4 years and then dying back down. It hit its all time peak this year in 2023, however, reaching a search frequency of 100 search hits. The term “Biden” had the lowest search hits, and had very minimal spikes in his trends.\n\n\n\n\n\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Assignment 3.html",
    "href": "Assignment 3.html",
    "title": "Assignment 3: Quanteda",
    "section": "",
    "text": "Biden-Xi Summit\nThe Biden-Xi summit was a virtual summit between U.S. President Joe Biden and Chinese President Xi Jinping, and was held on November 15 and 16, 2021. This summit was a critical diplomatic meeting between the leaders of the United States and China and aimed to address a range of important issues in the U.S.-China relationship, such as bilateral relations, economic and trade issues, human rights and climate change, regional and cyber security, and global issues such as COVID 19.\nWe used the provided R code and the Quanteda package to perform a text analysis on Twitter data regarding the Biden-Xi Summit. The following is what was identified from the analysis:\nThis line of code (head(tweet_dfm)) helps provide insight of the document terms in the data matrix.\n\n\n\n\n\nThis line of code (head(toptag, 10)) extracts the top 10 frequently used hashtags in the data matrix. As you can see, these hashtags are generically focused on the countries and presidents that the summit revolves around, but also touches on some global issues, such as COVID-19, drug usage, and the Uyghur Genocide.\n\n\n\n\n\nThese lines of code (head(tag_fcm)) explores the relationship between the hashtags, and how they are used together. This image shows the analysis on the first few rows of hashtags, and how they correspond together.\n\n\n\n\n\nThis network plot shows visually the relationship between the hashtags, and how they correspond with each other.\n\n\n\n\n\nThis line of code (head(topuser, 20)) extracts the top 20 frequent Twitter usernames found in the data matrix.\n\n\n\n\n\nThis image shows a network plot of a subset of Twitter usernames that were pulled from the data matrix, to show the visual relationship between usernames and their frequency of co-occurence in the matrix.\n\n\n\n\n\n\n\nU.S. Presidential Inaugural Speeches\nTo analyze Presidential Inaugural Speeches, I used the Quanteda package in R Studio to generate visualizations. The following is what I identified from my analysis:\n\n\n\n\n\nTo retrieve this image, we used the code ’kwic(tokens(data_corpus_inaugural_subset), pattern = “american”) %&gt;%\ntextplot_xray().\nBy using this code, we perform a keyword-in-context analysis and specifically search for the word “american” in the data matrix. It then generates the visualization so we can see the frequency in which the term “american” was used in each president’s inaugural speeches.\n\n\n\n\n\nWe use a similar code to derive this visualization, except we add in the functions to generate for the keywords “people” and “communist” as well. This visualization shows us how frequently each word was used in each president’s inaugural addresses, and how they compare to each of the other words as well.\nOver time and between presidents, the usage of the term “american” varied, but the usage of the term itself was not that high. The usage of the term “people” was a lot higher, and was used frequently in every president’s inaugural address.\n\n\nWhat is Wordfish?\nWordfish is a Quanteda Package function used in R Studio to analyze the relationship between words in documents. It is often used for text scaling, and will show the associated strength of each word within a document. To use this function, use ‘textstat_wordfish()’.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Assignment 6.html",
    "href": "Assignment 6.html",
    "title": "Assignment 6: Webscraping",
    "section": "",
    "text": "The textmining01.r code is a script for performing text mining and generating a word cloud from downloaded text data regarding Martin Luther King Jr.’s “I have a Dream” speech. The script downloads data using the “htmlTreeParse” function and stores it as a variable. The text is then pre-processed and vectorized, with the text being converted to all lower case, number and punctuation stripped away, and a matrix formed to represent the frequencies. The matrix is shown below:\n\n\n\n\n\nThe “wordcounts” variable is then used to order these frequencies in descending order, shown below:\n\n\n\n\n\nWith this, we can then formulate Word Clouds with our processed text data. The larger the word is presented in the Word Cloud, the more frequent it is presented in the speech.\n \n\n\nNext, we will conduct a similar analysis on Winston Churchill’s speech “The Finest Hour”. We will use the following full script to analyze the text.\n\n\nThe Word Cloud is presented below:\n\n\n\n\n\nHere we can see from our text analysis that words such as “war”, “france” and “french”, and “great” are very frequent in the speech."
  },
  {
    "objectID": "Assignment 6.html#their-finest-hour--winston-churchill",
    "href": "Assignment 6.html#their-finest-hour--winston-churchill",
    "title": "Assignment 6: Webscraping",
    "section": "",
    "text": "Next, we will conduct a similar analysis on Winston Churchill’s speech “The Finest Hour”. We will use the following full script to analyze the text.\n\n\nThe Word Cloud is presented below:\n\n\n\n\n\nHere we can see from our text analysis that words such as “war”, “france” and “french”, and “great” are very frequent in the speech."
  },
  {
    "objectID": "Assignment 7.html",
    "href": "Assignment 7.html",
    "title": "Assignment 7: Gov. Data & Parallel Processing",
    "section": "",
    "text": "With the R Scripts in govdata01.r and parallel01.r, we are attempting to download data sets from government info and clean the variables, exporting them out into individual data sets in PDF format. Unfortunately, however, my script repeatedly failed to download the PDFs of the government info data, as R kept informing me it could not read the table due to there being more columns than column names.\nHowever, I attempted the given exercise, which was to process data for \"118th Congress Congressional Hearings in Committee on Foreign Affairs?\" but a similar problem persisted. Due to this, I was unable to run a parallel computation using the parallel01.r script."
  },
  {
    "objectID": "Assignment6.html",
    "href": "Assignment6.html",
    "title": "Assignment 6: Webscraping",
    "section": "",
    "text": "The textmining01.r code is a script for performing text mining and generating a word cloud from downloaded text data regarding Martin Luther King Jr.’s “I have a Dream” speech. The script downloads data using the “htmlTreeParse” function and stores it as a variable. The text is then pre-processed and vectorized, with the text being converted to all lower case, number and punctuation stripped away, and a matrix formed to represent the frequencies. The matrix is shown below:\n\n\n\n\n\nThe “wordcounts” variable is then used to order these frequencies in descending order, shown below:\n\n\n\n\n\nWith this, we can then formulate Word Clouds with our processed text data. The larger the word is presented in the Word Cloud, the more frequent it is presented in the speech.\n \n\n\nNext, we will conduct a similar analysis on Winston Churchill’s speech “The Finest Hour”. We will use the following full script to analyze the text.\n\n\nThe Word Cloud is presented below:\n\n\n\n\n\nHere we can see from our text analysis that words such as “war”, “france” and “french”, and “great” are very frequent in the speech."
  },
  {
    "objectID": "Assignment6.html#their-finest-hour--winston-churchill",
    "href": "Assignment6.html#their-finest-hour--winston-churchill",
    "title": "Assignment 6: Webscraping",
    "section": "",
    "text": "Next, we will conduct a similar analysis on Winston Churchill’s speech “The Finest Hour”. We will use the following full script to analyze the text.\n\n\nThe Word Cloud is presented below:\n\n\n\n\n\nHere we can see from our text analysis that words such as “war”, “france” and “french”, and “great” are very frequent in the speech."
  },
  {
    "objectID": "Assignment8.html",
    "href": "Assignment8.html",
    "title": "Assignment 8: Census & Spatial Data",
    "section": "",
    "text": "In the spatialdata01.r code, we explore the median age by state in 2019 using an API Census key.\nThe code loads American Community Survey (ACS) 2019 variables using the “tidycensus::load_variables” function. It also loads ACS 2019 profile variables. The “get_acs” function retrieves ACS data for median age by state in 2019. The data is obtained at the state level, and the resulting graph is shown below.\n\n\n\n\n\nThe final map visualizes the median age by state in 2019, providing a geographical representation of the data. Each state is shaded according to its median age, with the darker colors indicating a higher median age and the lighter colors indicating a lower median age.\n\n\nThe given exercise in the spatialdata01.r file was to test the code to download 2009 and 2020 data as well and compare them. Rewriting the code to download 2009 data was simple, and the visual is provided below. However, downloading the 2020 data was not possible, as an error occurred reading “Error in get_acs():! The regular 1-year ACS for 2020 was not released and is not available in tidycensus.”\n\n\n\n\n\nAt first glances, there did not seem to be many differences between the 2009 and 2019 data, but there are definitely some changes present. For example, the median age in Montana has decreased over the decade, as well as in Minnesota, Pennsylvania, North Dakota and New Jersey. Realistically, the only place we see the median age increasing in is in the U.S. Territory."
  },
  {
    "objectID": "Assignment8.html#quarto",
    "href": "Assignment8.html#quarto",
    "title": "Assignment 8: Census & Spatial Data",
    "section": "",
    "text": "Quarto enables you\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Assignment3.html",
    "href": "Assignment3.html",
    "title": "Assignment 3: Quanteda",
    "section": "",
    "text": "Biden-Xi Summit\nThe Biden-Xi summit was a virtual summit between U.S. President Joe Biden and Chinese President Xi Jinping, and was held on November 15 and 16, 2021. This summit was a critical diplomatic meeting between the leaders of the United States and China and aimed to address a range of important issues in the U.S.-China relationship, such as bilateral relations, economic and trade issues, human rights and climate change, regional and cyber security, and global issues such as COVID 19.\nWe used the provided R code and the Quanteda package to perform a text analysis on Twitter data regarding the Biden-Xi Summit. The following is what was identified from the analysis:\nThis line of code (head(tweet_dfm)) helps provide insight of the document terms in the data matrix.\n\n\n\n\n\nThis line of code (head(toptag, 10)) extracts the top 10 frequently used hashtags in the data matrix. As you can see, these hashtags are generically focused on the countries and presidents that the summit revolves around, but also touches on some global issues, such as COVID-19, drug usage, and the Uyghur Genocide.\n\n\n\n\n\nThese lines of code (head(tag_fcm)) explores the relationship between the hashtags, and how they are used together. This image shows the analysis on the first few rows of hashtags, and how they correspond together.\n\n\n\n\n\nThis network plot shows visually the relationship between the hashtags, and how they correspond with each other.\n\n\n\n\n\nThis line of code (head(topuser, 20)) extracts the top 20 frequent Twitter usernames found in the data matrix.\n\n\n\n\n\nThis image shows a network plot of a subset of Twitter usernames that were pulled from the data matrix, to show the visual relationship between usernames and their frequency of co-occurence in the matrix.\n\n\n\n\n\n\n\nU.S. Presidential Inaugural Speeches\nTo analyze Presidential Inaugural Speeches, I used the Quanteda package in R Studio to generate visualizations. The following is what I identified from my analysis:\n\n\n\n\n\nTo retrieve this image, we used the code ’kwic(tokens(data_corpus_inaugural_subset), pattern = “american”) %&gt;%\ntextplot_xray().\nBy using this code, we perform a keyword-in-context analysis and specifically search for the word “american” in the data matrix. It then generates the visualization so we can see the frequency in which the term “american” was used in each president’s inaugural speeches.\n\n\n\n\n\nWe use a similar code to derive this visualization, except we add in the functions to generate for the keywords “people” and “communist” as well. This visualization shows us how frequently each word was used in each president’s inaugural addresses, and how they compare to each of the other words as well.\nOver time and between presidents, the usage of the term “american” varied, but the usage of the term itself was not that high. The usage of the term “people” was a lot higher, and was used frequently in every president’s inaugural address.\n\n\nWhat is Wordfish?\nWordfish is a Quanteda Package function used in R Studio to analyze the relationship between words in documents. It is often used for text scaling, and will show the associated strength of each word within a document. To use this function, use ‘textstat_wordfish()’.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Assignment7.html",
    "href": "Assignment7.html",
    "title": "Assignment 7: Gov. Data & Parallel Processing",
    "section": "",
    "text": "Gov. Data & Parallel Processing\nWith the R Scripts in govdata01.r and parallel01.r, we are attempting to download data sets from government info and clean the variables, exporting them out into individual data sets in PDF format. Unfortunately, however, my script repeatedly failed to download the PDF’s of the government info data, as R kept informing me it could not read the table due to there being more columns than column names.\n\n\n\n\n\nHowever, I attempted the given exercise, which was to process data for “118th Congress Congressional Hearings in Committee on Foreign Affairs?” but a similar problem persisted. Due to this, I was unable to run a parallel computation using the parallel01.r script.\n\nStorage Planning\nTools like “tic tac” are crucial when dealing with mass downloads of data, as this tool helps scale data to prevent the overwhelming responses that comes with the downloading. Effective storage planning is important to see success in your code, as otherwise the download could occupy your entire hard drive. It’s also important to keep files organized within your hard drive to ensure easy access to locate the downloaded files.\n\n\nAdditional Storage Methods\nArrow: Arrow is a columnar, in-memory data format that facilitates fast analytics on large datasets. In R, the Arrow package provides an interface for reading and writing Arrow data. With efficient memory utilization and support for various data types, Arrow enables seamless data interchange between different systems, making it an excellent choice for high-performance computing tasks.\nFeather: Feather is a binary columnar data format designed for optimal performance and compatibility. In R, the feather package allows users to read and write data in the Feather format. Feather excels in terms of speed and efficiency, making it suitable for scenarios where quick and reliable data exchange is crucial. Its simplicity and cross-language support make it an attractive option for data scientists working on diverse projects.\nParquet: Parquet is a columnar storage file format that is highly optimized for use with big data processing frameworks. In R, the parquet package facilitates the reading and writing of data in the Parquet format. Parquet is known for its compression capabilities and schema evolution support, making it well-suited for data-intensive applications. Its compatibility with various programming languages and big data tools enhances its versatility in large-scale data processing workflows.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Assignment2.html",
    "href": "Assignment2.html",
    "title": "Assignment 2: Google Trends Data",
    "section": "",
    "text": "I used Google Trends and the gtrendsR Package to analyze data regarding the terms “Trump, Biden and Election”.\nWhen analyzing these terms on Google Trends, I noticed that the term “election” peaked on November 6, 2022, while the terms “Trump” and “Biden” stayed relatively stable all throughout 2022 and 2023. The term “Trump” had several hits around April and August of 2023, but it is safe to infer that it was not in correlation with the term “election”, as this was likely due to Trump’s presence in court cases at the time.\n\n\n\n\n\nUsing the gtrends package in R Studio yielded different results. The graph displayed by R studio shows that “Trump” was a popular term searched since 2005, but the term started picking up in search frequency around the 2016 election time. His next peak was in 2023, again likely due to his ongoing court cases. The term “election”, however was the most consistent in its peak, spiking every 4 years and then dying back down. It hit its all time peak this year in 2023, however, reaching a search frequency of 100 search hits. The term “Biden” had the lowest search hits, and had very minimal spikes in his trends.\n\n\n\n\n\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Assignment1.html",
    "href": "Assignment1.html",
    "title": "Assignment 1: Qualtrics Survey",
    "section": "",
    "text": "Link to survey: https://utdallas.qualtrics.com/jfe/form/SV_8qYDz8fe1fd8nZQ\nSurvey Analysis:\n\nThe survey is structured linearly to collect data and gauge responses on how frequently the sample population watch movies, and what content in movies they prefer. This is asked before addressing the demographics of the sample population.\nThe questionnaire is composed of multiple choice questions (objective questions), as well as Likert Scales questions. This is used to collect data based on the opinions of the sample population.\nThe questions are ordered by starting with a Likert Scales question, that then continues into multiple choice questions based on the opinions of the topic. The questionnaire concludes by collecting information on the sample population’s demographics.\n\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "CSV.html",
    "href": "CSV.html",
    "title": "My Resume",
    "section": "",
    "text": "469.275.3300; jhanvi_kannan@yahoo.com\nhttp://www.linkedin.com/in/jhanvikannan\n\n\n\nThe University of Texas at Dallas\nM.S.,Social Data Analytics and Research — August 2023 – December 2024\nB.S., Business Administration in Innovation and Entrepreneurship — Graduated May 2023 \nAcademic Excellence Scholarship (AES) Recipient\n\n\n\nGEICO- Richardson, Texas — June 6, 2022 – July 29, 2022\nSummer Business Leadership Intern\n· Conducted in-depth research on insurance trends and collected renewal data for underwriting submissions.\n· Collaborated with peers and completed a project to improve work culture in a post-pandemic environment.\n· Collaborated with peers and completed a project on the implementation of an AI chat system to enhance customer interactions.\n· Created training modules for employees to ensure a seamless transition to the new chat system, contributing to improved customer experiences.\n· Participated in research and training of insurance policies within GEICO’s regulations.\n· Maintained client confidentiality and ensured data accuracy.\nEno’s Pizza Tavern- Cypress Waters, Texas — May 2019 – August 2019, June 2020 – October 2021\nShift Manager\n· Oversaw restaurant operations, including opening, closing, and front/back-of-house management and training.\n· Conducted comprehensive training for FOH staff, ensuring proficiency in food and beverage knowledge.\n· Managed payroll, nightly and weekly audits, and sales reports.\n· Implemented weekly audits to forecast income accurately.\n\n\n\n· Proficient in Photoshop, Microsoft Software, Salesforce, SQL, Tableau, R, and Python.\n· Business & Public Law, Professional Development, Financial Accounting, Principles of Marketing, Management Methods of Decision Making, Management Accounting, Operations Management, Business Finance, Digital Prospecting, Methods of Data Collection, Research Design, Business Statistics.\n\n\n\n· Effective communication, Teamwork, Problem-Solving, Social Media Marketing, Leadership, Fundraising\n· Eligibility: (USPR) Eligible to work in the U.S. with no restrictions\n\n\n\nAwaazein, South Asian A Cappella Competition || Executive Director\n· Coordinated a National A Cappella Competition, managing a budget of $18,000 and achieving a profit.\n· Developed a virtual voting system for team selection to compete.\n· Secured sponsorships from over 50 local vendors and business.\nAkshaya Patra, Non-Profit Organization || President\n· Guided a non-profit organization that helps feed impoverished kids in India, while helping fund their education.\n· Raised $5,700 in one academic year to provide for impoverished children, while also bringing Indian cultural activities to UTD Student life.\n· Implemented Akshaya Patra as an official NGO Partner for a national A Cappella competition and a national classical dance competition.\n· Planned and executed monthly fundraising activities with 100+ active members.\nHansini, Classical Dance Competition || Marketing/PR Chair\n· Promoted UTD’s classical dance competition through social media and outreach.\n· Utilized Photoshop and Canva for content creation.\nAaja Nachle, South Asian Dance Competition || Registration & Finance Chair\n· Implemented an online anonymous voting system for team selection to compete.\n· Managed logistics and funding for the 8 competing teams, and organized fundraising efforts at UTD.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "CSV.html#quarto",
    "href": "CSV.html#quarto",
    "title": "csv",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "CSV.html#running-code",
    "href": "CSV.html#running-code",
    "title": "csv",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "CSV.html#jhanvi-kannan",
    "href": "CSV.html#jhanvi-kannan",
    "title": "My Resume",
    "section": "",
    "text": "469.275.3300; jhanvi_kannan@yahoo.com\nhttp://www.linkedin.com/in/jhanvikannan\n\n\n\nThe University of Texas at Dallas\nM.S.,Social Data Analytics and Research — August 2023 – December 2024\nB.S., Business Administration in Innovation and Entrepreneurship — Graduated May 2023 \nAcademic Excellence Scholarship (AES) Recipient\n\n\n\nGEICO- Richardson, Texas — June 6, 2022 – July 29, 2022\nSummer Business Leadership Intern\n· Conducted in-depth research on insurance trends and collected renewal data for underwriting submissions.\n· Collaborated with peers and completed a project to improve work culture in a post-pandemic environment.\n· Collaborated with peers and completed a project on the implementation of an AI chat system to enhance customer interactions.\n· Created training modules for employees to ensure a seamless transition to the new chat system, contributing to improved customer experiences.\n· Participated in research and training of insurance policies within GEICO’s regulations.\n· Maintained client confidentiality and ensured data accuracy.\nEno’s Pizza Tavern- Cypress Waters, Texas — May 2019 – August 2019, June 2020 – October 2021\nShift Manager\n· Oversaw restaurant operations, including opening, closing, and front/back-of-house management and training.\n· Conducted comprehensive training for FOH staff, ensuring proficiency in food and beverage knowledge.\n· Managed payroll, nightly and weekly audits, and sales reports.\n· Implemented weekly audits to forecast income accurately.\n\n\n\n· Proficient in Photoshop, Microsoft Software, Salesforce, SQL, Tableau, R, and Python.\n· Business & Public Law, Professional Development, Financial Accounting, Principles of Marketing, Management Methods of Decision Making, Management Accounting, Operations Management, Business Finance, Digital Prospecting, Methods of Data Collection, Research Design, Business Statistics.\n\n\n\n· Effective communication, Teamwork, Problem-Solving, Social Media Marketing, Leadership, Fundraising\n· Eligibility: (USPR) Eligible to work in the U.S. with no restrictions\n\n\n\nAwaazein, South Asian A Cappella Competition || Executive Director\n· Coordinated a National A Cappella Competition, managing a budget of $18,000 and achieving a profit.\n· Developed a virtual voting system for team selection to compete.\n· Secured sponsorships from over 50 local vendors and business.\nAkshaya Patra, Non-Profit Organization || President\n· Guided a non-profit organization that helps feed impoverished kids in India, while helping fund their education.\n· Raised $5,700 in one academic year to provide for impoverished children, while also bringing Indian cultural activities to UTD Student life.\n· Implemented Akshaya Patra as an official NGO Partner for a national A Cappella competition and a national classical dance competition.\n· Planned and executed monthly fundraising activities with 100+ active members.\nHansini, Classical Dance Competition || Marketing/PR Chair\n· Promoted UTD’s classical dance competition through social media and outreach.\n· Utilized Photoshop and Canva for content creation.\nAaja Nachle, South Asian Dance Competition || Registration & Finance Chair\n· Implemented an online anonymous voting system for team selection to compete.\n· Managed logistics and funding for the 8 competing teams, and organized fundraising efforts at UTD.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "epps6313.html",
    "href": "epps6313.html",
    "title": "EPPS6313: Research Paper",
    "section": "",
    "text": "Below is a research paper that I collaborated on with a team for my Introduction to Quantitative Methods class. This research delves into the correlation between access to mental health providers across the United States and its impact on rates of teen pregnancy, high school graduation, and juvenile arrests."
  },
  {
    "objectID": "epps6302.html",
    "href": "epps6302.html",
    "title": "EPPS 6302: Research Paper",
    "section": "",
    "text": "This is our final project for Methods of Data Collection and Production. Our study delves into the repercussions of climate change on the well-being of Dallas-Fort Worth (DFW) residents and examines the adaptive measures adopted by the community to navigate the persistent challenges in the foreseeable future.\nClick here to view our Final Project Slides"
  },
  {
    "objectID": "Assignment8.html#spatialdata01.r",
    "href": "Assignment8.html#spatialdata01.r",
    "title": "Assignment 8: Census & Spatial Data",
    "section": "",
    "text": "In the spatialdata01.r code, we explore the median age by state in 2019 using an API Census key.\nThe code loads American Community Survey (ACS) 2019 variables using the “tidycensus::load_variables” function. It also loads ACS 2019 profile variables. The “get_acs” function retrieves ACS data for median age by state in 2019. The data is obtained at the state level, and the resulting graph is shown below.\n\n\n\n\n\nThe final map visualizes the median age by state in 2019, providing a geographical representation of the data. Each state is shaded according to its median age, with the darker colors indicating a higher median age and the lighter colors indicating a lower median age.\n\n\nThe given exercise in the spatialdata01.r file was to test the code to download 2009 and 2020 data as well and compare them. Rewriting the code to download 2009 data was simple, and the visual is provided below. However, downloading the 2020 data was not possible, as an error occurred reading “Error in get_acs():! The regular 1-year ACS for 2020 was not released and is not available in tidycensus.”\n\n\n\n\n\nAt first glances, there did not seem to be many differences between the 2009 and 2019 data, but there are definitely some changes present. For example, the median age in Montana has decreased over the decade, as well as in Minnesota, Pennsylvania, North Dakota and New Jersey. Realistically, the only place we see the median age increasing in is in the U.S. Territory."
  },
  {
    "objectID": "Assignment8.html#spatialdata02.r",
    "href": "Assignment8.html#spatialdata02.r",
    "title": "Assignment 8: Census & Spatial Data",
    "section": "spatialdata02.r",
    "text": "spatialdata02.r\nNext, we run the spatialdata02.r code. This code uses the “get_acs” function to retrieve income data for all tracts in Texas (state code “TX”) for the year 2020. The income data is then plotted and the result is a visualization of income estimates at the tract level for both Texas and Dallas County in the year 2020, providing insights into income distribution across geographical areas. The “tmap” and “mapview” functions enable users to create thematic maps and interactively explore the data.\n  \n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "6323HW2.html",
    "href": "6323HW2.html",
    "title": "Assignment 2: Brush Up R and Quarto",
    "section": "",
    "text": "x &lt;- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)\n\n\n\n\n\nlength(x)  # What does length() do?\n\n[1] 3\n\nlength(y)\n\n[1] 3\n\n\n\n\n\n\nx+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"has_annotations\" \"x\"               \"y\"              \n\nrm(x,y) # Remove objects\nls()\n\n[1] \"has_annotations\"\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!\n\n\n\n\n\n?matrix\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9935692\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)\n\n\n\n\n\nmean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768\n\n\n\n\n\n\nx=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\nquartz_off_screen \n                2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "6323HW2.html#create-object-using-the-assignment-operator--",
    "href": "6323HW2.html#create-object-using-the-assignment-operator--",
    "title": "Assignment 2: Brush Up R and Quarto",
    "section": "",
    "text": "x &lt;- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)"
  },
  {
    "objectID": "6323HW2.html#using-function",
    "href": "6323HW2.html#using-function",
    "title": "Assignment 2: Brush Up R and Quarto",
    "section": "",
    "text": "length(x)  # What does length() do?\n\n[1] 3\n\nlength(y)\n\n[1] 3"
  },
  {
    "objectID": "6323HW2.html#using---operators",
    "href": "6323HW2.html#using---operators",
    "title": "Assignment 2: Brush Up R and Quarto",
    "section": "",
    "text": "x+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"has_annotations\" \"x\"               \"y\"              \n\nrm(x,y) # Remove objects\nls()\n\n[1] \"has_annotations\"\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!"
  },
  {
    "objectID": "6323HW2.html#matrix-operations",
    "href": "6323HW2.html#matrix-operations",
    "title": "Assignment 2: Brush Up R and Quarto",
    "section": "",
    "text": "?matrix\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9935692\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)"
  },
  {
    "objectID": "6323HW2.html#simple-descriptive-statistics-base",
    "href": "6323HW2.html#simple-descriptive-statistics-base",
    "title": "Assignment 2: Brush Up R and Quarto",
    "section": "",
    "text": "mean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768"
  },
  {
    "objectID": "6323HW2.html#visualization-using-r-graphics-without-packages",
    "href": "6323HW2.html#visualization-using-r-graphics-without-packages",
    "title": "Assignment 2: Brush Up R and Quarto",
    "section": "",
    "text": "x=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\nquartz_off_screen \n                2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "6323HW2.html#indexing-data-using",
    "href": "6323HW2.html#indexing-data-using",
    "title": "Assignment 2: Brush Up R and Quarto",
    "section": "Indexing Data using []",
    "text": "Indexing Data using []\n\nA=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),] # What does -c() do?\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A) # Dimensions\n\n[1] 4 4"
  },
  {
    "objectID": "6323HW2.html#loading-data-from-github-remote",
    "href": "6323HW2.html#loading-data-from-github-remote",
    "title": "Assignment 2: Brush Up R and Quarto",
    "section": "Loading Data from GitHub (remote)",
    "text": "Loading Data from GitHub (remote)\n\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\",header=T,na.strings=\"?\") \nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\") # read csv file\n# Which function reads data faster?\n\n# Try using this simple method\n# time1 = proc.time()\n# Auto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\",header=T,na.strings=\"?\")\n# proc.time()-time1\n\n# Check on data\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,] # select rows\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto) # Notice the difference?\n\n[1] 397   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\""
  },
  {
    "objectID": "6323HW2.html#load-data-from-islr-website",
    "href": "6323HW2.html#load-data-from-islr-website",
    "title": "Assignment 2: Brush Up R and Quarto",
    "section": "Load data from ISLR website",
    "text": "Load data from ISLR website\n\nAuto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9"
  },
  {
    "objectID": "6323HW2.html#additional-graphical-and-numerical-summaries",
    "href": "6323HW2.html#additional-graphical-and-numerical-summaries",
    "title": "Assignment 2: Brush Up R and Quarto",
    "section": "Additional Graphical and Numerical Summaries",
    "text": "Additional Graphical and Numerical Summaries\n\n# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\nhist(mpg)\n\n\n\nhist(mpg,col=2)\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\nplot(horsepower,mpg)\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60"
  },
  {
    "objectID": "6323HW2.html#linear-regression",
    "href": "6323HW2.html#linear-regression",
    "title": "Assignment 2: Brush Up R and Quarto",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\n\nThe downloaded binary packages are in\n    /var/folders/n8/p8ct1r_92t92z2qhc173_sqr0000gn/T//Rtmp0JWk3P/downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nLoading required package: ISLR\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375"
  },
  {
    "objectID": "6323HW2.html#multiple-linear-regression",
    "href": "6323HW2.html#multiple-linear-regression",
    "title": "Assignment 2: Brush Up R and Quarto",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\nlm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)"
  },
  {
    "objectID": "6323HW2.html#non-linear-transformations-of-the-predictors",
    "href": "6323HW2.html#non-linear-transformations-of-the-predictors",
    "title": "Assignment 2: Brush Up R and Quarto",
    "section": "Non-linear Transformations of the Predictors",
    "text": "Non-linear Transformations of the Predictors\n\nlm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "6323HW2.html#qualitative-predictors",
    "href": "6323HW2.html#qualitative-predictors",
    "title": "Assignment 2: Brush Up R and Quarto",
    "section": "Qualitative Predictors",
    "text": "Qualitative Predictors\n\n# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1"
  },
  {
    "objectID": "6323HW2.html#interaction-terms-including-interaction-and-single-effects",
    "href": "6323HW2.html#interaction-terms-including-interaction-and-single-effects",
    "title": "Assignment 2: Brush Up R and Quarto",
    "section": "Interaction Terms (including interaction and single effects)",
    "text": "Interaction Terms (including interaction and single effects)\n\nsummary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "6323HW2.html#problems-and-missing-values",
    "href": "6323HW2.html#problems-and-missing-values",
    "title": "Assignment 2: Brush Up R and Quarto",
    "section": "Problems and Missing Values",
    "text": "Problems and Missing Values\nSome problems present in the dataset is that it is difficult to read. In Stata, we are able to use the code “codebook” to look at what the variable labels represent, but in R, it is difficult to do so. We see 1,690 entries, but are unable to identify what each value represents.\nTo identify missingness in large datasets, we can use visual methods to identify any gaps. Then we can exclude observations with missing values entirely from the analysis, if the missing values are not too prevalent and removing them does not significantly impact the analysis."
  },
  {
    "objectID": "6323HW2.html#tondu-and-other-variables",
    "href": "6323HW2.html#tondu-and-other-variables",
    "title": "Assignment 2: Brush Up R and Quarto",
    "section": "Tondu and other Variables",
    "text": "Tondu and other Variables\nTo explore the relationship between the “Tondu” variable and other variables including “female”, “DPP”, “age”, “income”, “edu”, “Taiwanese”, and “Econ_worse”, you can use various statistical methods such as:\n\nDescriptive statistics: Calculate summary statistics (mean, median, standard deviation, etc.) for each variable and examine patterns.\n\n\n\nVisualization: Create visualizations such as scatter plots, histograms, box plots, or heatmaps to visualize the relationships between variables.\nCorrelation analysis: Compute correlation coefficients (e.g., Pearson correlation, Spearman correlation) to quantify the strength and direction of the relationships between continuous variables."
  },
  {
    "objectID": "6323HW2.html#the-votetsai-variable",
    "href": "6323HW2.html#the-votetsai-variable",
    "title": "Assignment 2: Brush Up R and Quarto",
    "section": "The “votetsai” Variable",
    "text": "The “votetsai” Variable\nTo explore the “votetsai” variable (vote for DPP candidate Tsai Ing-wen), you can use similar methods as mentioned above:\n\nDescriptive statistics: Examine the distribution of votes for Tsai Ing-wen and calculate proportions or percentages.\nVisualization: Create bar plots or pie charts to visualize the distribution of votes for Tsai Ing-wen among different groups.\nCross-tabulation: Cross-tabulate “votetsai” with other relevant variables to explore any associations or patterns.\nStatistical tests: Conduct statistical tests (e.g., chi-square test) to examine whether there are significant differences in voting behavior across different demographic or socioeconomic groups."
  },
  {
    "objectID": "6323HW2.html#tondu-variable-frequency-and-bar-chart",
    "href": "6323HW2.html#tondu-variable-frequency-and-bar-chart",
    "title": "Assignment 2: Brush Up R and Quarto",
    "section": "“Tondu” Variable Frequency and Bar Chart",
    "text": "“Tondu” Variable Frequency and Bar Chart\n\nTEDS_2016$Tondu &lt;- factor(TEDS_2016$Tondu,\n                           labels = c(\"Unification now\", \"Status quo, unif. in future\", \n                                      \"Status quo, decide later\", \"Status quo forever\", \n                                      \"Status quo, indep. in future\", \"Independence now\", \n                                      \"No response\"))\n\nfrequency_table &lt;- table(TEDS_2016$Tondu)\nprint(frequency_table)\n\n\n             Unification now  Status quo, unif. in future \n                          27                          180 \n    Status quo, decide later           Status quo forever \n                         546                          328 \nStatus quo, indep. in future             Independence now \n                         380                          108 \n                 No response \n                         121 \n\nbarplot(frequency_table, \n        main = \"Frequency of Tondu Responses\",\n        xlab = \"Tondu Responses\",\n        ylab = \"Frequency\",\n        col = \"skyblue\",\n        ylim = c(0, max(frequency_table) + 100),  # Adjust ylim for better visualization\n        las = 2)  # Rotate x-axis labels for better readability"
  },
  {
    "objectID": "6323HW3.html",
    "href": "6323HW3.html",
    "title": "Assignment 3: Exploratory Data Analysis",
    "section": "",
    "text": "## Creating a function: regplot\n## Combine the lm, plot and abline functions to create a regression fit plot function\nregplot=function(x,y){\n  fit=lm(y~x)\n  plot(x,y)\n  abline(fit,col=\"red\")\n}\n\n\nattach(ISLR::Carseats)\nregplot(Price,Sales)\n\n\n\n## Allow extra room for additional arguments/specifications\nregplot=function(x,y,...){\n  fit=lm(y~x)\n  plot(x,y,...)\n  abline(fit,col=\"red\")\n}  # \"...\" is called ellipsis, which is designed to take any number of named or unnamed arguments.\nregplot(Price,Sales,xlab=\"Price\",ylab=\"Sales\",col=\"blue\",pch=20)\n\n\n\n\n(Adapted from Stackoverflow examples) (Objectives: Use plotly, reshape packages, interactive visualization)\n\nlibrary(tidyverse)\nlibrary(plotly)\ndata(iris)\nattach(iris)\n# Generate plot on three quantitative variables\niris_plot &lt;- plot_ly(iris,\n                     x = Sepal.Length,\n                     y = Sepal.Width,\n                     z = Petal.Length,\n                     type = \"scatter3d\",\n                     mode = \"markers\",\n                     size = 0.02)\niris_plot\n\n\n\n\n# Regression object\n\npetal_lm &lt;- lm(Petal.Length ~ 0 + Sepal.Length + Sepal.Width,\n               data = iris)\nlibrary(reshape2)\n\n#load data\n\npetal_lm &lt;- lm(Petal.Length ~ 0 + Sepal.Length + Sepal.Width,data = iris)\n\n# Setting resolution parameter\ngraph_reso &lt;- 0.05\n\n#Setup Axis\naxis_x &lt;- seq(min(iris$Sepal.Length), max(iris$Sepal.Length), by = graph_reso)\naxis_y &lt;- seq(min(iris$Sepal.Width), max(iris$Sepal.Width), by = graph_reso)\n\n# Regression surface\n# Rearranging data for plotting\npetal_lm_surface &lt;- expand.grid(Sepal.Length = axis_x,Sepal.Width = axis_y,KEEP.OUT.ATTRS = F)\npetal_lm_surface$Petal.Length &lt;- predict.lm(petal_lm, newdata = petal_lm_surface)\npetal_lm_surface &lt;- acast(petal_lm_surface, Sepal.Width ~ Sepal.Length, value.var = \"Petal.Length\")\nhcolors=c(\"orange\",\"blue\",\"green\")[iris$Species]\niris_plot &lt;- plot_ly(iris,\n                     x = ~Sepal.Length,\n                     y = ~Sepal.Width,\n                     z = ~Petal.Length,\n                     text = Species,\n                     type = \"scatter3d\",\n                     mode = \"markers\",\n                     marker = list(color = hcolors),\n                     size=0.02)\n# Add surface\niris_plot &lt;- add_trace(p = iris_plot,\n                       z = petal_lm_surface,\n                       x = axis_x,\n                       y = axis_y,\n                       type = \"surface\",mode = \"markers\",\n                       marker = list(color = hcolors))\niris_plot\n\n\n\n\n\n\n\n\n\npetal_lm &lt;- lm(Petal.Length ~ 0 + Sepal.Length + Sepal.Width, \n               data = iris)\nsummary(petal_lm)\n\n\nCall:\nlm(formula = Petal.Length ~ 0 + Sepal.Length + Sepal.Width, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.70623 -0.51867 -0.08334  0.49844  1.93093 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \nSepal.Length  1.56030    0.04557   34.24   &lt;2e-16 ***\nSepal.Width  -1.74570    0.08709  -20.05   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6869 on 148 degrees of freedom\nMultiple R-squared:  0.973, Adjusted R-squared:  0.9726 \nF-statistic:  2663 on 2 and 148 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "6323HW3.html#r-programming-eda",
    "href": "6323HW3.html#r-programming-eda",
    "title": "Assignment 3: Exploratory Data Analysis",
    "section": "",
    "text": "## Creating a function: regplot\n## Combine the lm, plot and abline functions to create a regression fit plot function\nregplot=function(x,y){\n  fit=lm(y~x)\n  plot(x,y)\n  abline(fit,col=\"red\")\n}\n\n\nattach(ISLR::Carseats)\nregplot(Price,Sales)\n\n\n\n## Allow extra room for additional arguments/specifications\nregplot=function(x,y,...){\n  fit=lm(y~x)\n  plot(x,y,...)\n  abline(fit,col=\"red\")\n}  # \"...\" is called ellipsis, which is designed to take any number of named or unnamed arguments.\nregplot(Price,Sales,xlab=\"Price\",ylab=\"Sales\",col=\"blue\",pch=20)\n\n\n\n\n(Adapted from Stackoverflow examples) (Objectives: Use plotly, reshape packages, interactive visualization)\n\nlibrary(tidyverse)\nlibrary(plotly)\ndata(iris)\nattach(iris)\n# Generate plot on three quantitative variables\niris_plot &lt;- plot_ly(iris,\n                     x = Sepal.Length,\n                     y = Sepal.Width,\n                     z = Petal.Length,\n                     type = \"scatter3d\",\n                     mode = \"markers\",\n                     size = 0.02)\niris_plot\n\n\n\n\n# Regression object\n\npetal_lm &lt;- lm(Petal.Length ~ 0 + Sepal.Length + Sepal.Width,\n               data = iris)\nlibrary(reshape2)\n\n#load data\n\npetal_lm &lt;- lm(Petal.Length ~ 0 + Sepal.Length + Sepal.Width,data = iris)\n\n# Setting resolution parameter\ngraph_reso &lt;- 0.05\n\n#Setup Axis\naxis_x &lt;- seq(min(iris$Sepal.Length), max(iris$Sepal.Length), by = graph_reso)\naxis_y &lt;- seq(min(iris$Sepal.Width), max(iris$Sepal.Width), by = graph_reso)\n\n# Regression surface\n# Rearranging data for plotting\npetal_lm_surface &lt;- expand.grid(Sepal.Length = axis_x,Sepal.Width = axis_y,KEEP.OUT.ATTRS = F)\npetal_lm_surface$Petal.Length &lt;- predict.lm(petal_lm, newdata = petal_lm_surface)\npetal_lm_surface &lt;- acast(petal_lm_surface, Sepal.Width ~ Sepal.Length, value.var = \"Petal.Length\")\nhcolors=c(\"orange\",\"blue\",\"green\")[iris$Species]\niris_plot &lt;- plot_ly(iris,\n                     x = ~Sepal.Length,\n                     y = ~Sepal.Width,\n                     z = ~Petal.Length,\n                     text = Species,\n                     type = \"scatter3d\",\n                     mode = \"markers\",\n                     marker = list(color = hcolors),\n                     size=0.02)\n# Add surface\niris_plot &lt;- add_trace(p = iris_plot,\n                       z = petal_lm_surface,\n                       x = axis_x,\n                       y = axis_y,\n                       type = \"surface\",mode = \"markers\",\n                       marker = list(color = hcolors))\niris_plot"
  },
  {
    "objectID": "6323HW3.html#regression-object",
    "href": "6323HW3.html#regression-object",
    "title": "Assignment 3: Exploratory Data Analysis",
    "section": "",
    "text": "petal_lm &lt;- lm(Petal.Length ~ 0 + Sepal.Length + Sepal.Width, \n               data = iris)\nsummary(petal_lm)\n\n\nCall:\nlm(formula = Petal.Length ~ 0 + Sepal.Length + Sepal.Width, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.70623 -0.51867 -0.08334  0.49844  1.93093 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \nSepal.Length  1.56030    0.04557   34.24   &lt;2e-16 ***\nSepal.Width  -1.74570    0.08709  -20.05   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6869 on 148 degrees of freedom\nMultiple R-squared:  0.973, Adjusted R-squared:  0.9726 \nF-statistic:  2663 on 2 and 148 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "6323HW4.html",
    "href": "6323HW4.html",
    "title": "Assignment 4: Unsupervised Learning",
    "section": "",
    "text": "library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(RColorBrewer)\n\ncomputers = read.csv(\"https://raw.githubusercontent.com/guru99-edu/R-Programming/master/computers.csv\") \n\n# Only retain two variables for illustration\nrescaled_comp &lt;- computers[4:5] %&gt;%\n  mutate(hd_scal = scale(hd),\n         ram_scal = scale(ram)) %&gt;%\n  select(c(hd_scal, ram_scal))\n        \nggplot(data = rescaled_comp, aes(x = hd_scal, y = ram_scal)) +\n  geom_point(pch=20, col = \"blue\") + theme_bw() +\n  labs(x = \"Hard drive size (Scaled)\", y =\"RAM size (Scaled)\" ) +\n  theme(text = element_text(family=\"Georgia\")) \n\n\n\n# install.packages(\"animation\")\nlibrary(animation)\nset.seed(2345)\nlibrary(animation)\n\n# Animate the K-mean clustering process, cluster no. = 4\nkmeans.ani(rescaled_comp[1:2], centers = 4, pch = 15:18, col = 1:4) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Iris example\n\n# Without grouping by species\nggplot(iris, aes(Petal.Length, Petal.Width)) + geom_point() + \n  theme_bw() +\n  scale_color_manual(values=c(\"firebrick1\",\"forestgreen\",\"darkblue\"))\n\n\n\n# With grouping by species\nggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point() + \n  theme_bw() +\n  scale_color_manual(values=c(\"firebrick1\",\"forestgreen\",\"darkblue\"))\n\n\n\n# Check k-means clusters\n## Starting with three clusters and 20 initial configurations\nset.seed(20)\nirisCluster &lt;- kmeans(iris[, 3:4], 3, nstart = 20)\nirisCluster\n\nK-means clustering with 3 clusters of sizes 52, 48, 50\n\nCluster means:\n  Petal.Length Petal.Width\n1     4.269231    1.342308\n2     5.595833    2.037500\n3     1.462000    0.246000\n\nClustering vector:\n  [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n [38] 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [75] 1 1 1 2 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 2 2 2 2\n[112] 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2\n[149] 2 2\n\nWithin cluster sum of squares by cluster:\n[1] 13.05769 16.29167  2.02200\n (between_SS / total_SS =  94.3 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\nclass(irisCluster$cluster)\n\n[1] \"integer\"\n\n# Confusion matrix\ntable(irisCluster$cluster, iris$Species)\n\n   \n    setosa versicolor virginica\n  1      0         48         4\n  2      0          2        46\n  3     50          0         0\n\nirisCluster$cluster &lt;- as.factor(irisCluster$cluster)\nggplot(iris, aes(Petal.Length, Petal.Width, color = irisCluster$cluster)) + geom_point() +\n  scale_color_manual(values=c(\"firebrick1\",\"forestgreen\",\"darkblue\")) +\n  theme_bw()\n\n\n\nactual = ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point() + \n  theme_bw() +\n  scale_color_manual(values=c(\"firebrick1\",\"forestgreen\",\"darkblue\")) +\n  theme(legend.position=\"bottom\") +\n  theme(text = element_text(family=\"Georgia\")) \nkmc = ggplot(iris, aes(Petal.Length, Petal.Width, color = irisCluster$cluster)) + geom_point() +\n  theme_bw() +\n  scale_color_manual(values=c(\"firebrick1\", \"darkblue\", \"forestgreen\")) +\n  theme(legend.position=\"bottom\") +\n  theme(text = element_text(family=\"Georgia\")) \nlibrary(grid)\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\ngrid.arrange(arrangeGrob(actual, kmc, ncol=2, widths=c(1,1)), nrow=1)\n\n\n\n\nThis R code performs various analyses and visualizations using unsupervised learning techniques, particularly focusing on clustering algorithms such as k-means and hierarchical clustering.\nIt creates a scatter plot using ggplot for the rescaled computer data (rescaled_comp) showing the relationship between hard drive size and RAM size and it animates the k-means clustering process for the rescaled computer data (rescaled_comp) with four clusters using the kmeans.ani function.\nFor the Iris dataset analysis, it creates scatter plots of petal length against petal width for the Iris dataset, both without and with coloring by species. It performs k-means clustering on the Iris dataset (iris) with three clusters and 20 initial configurations using the kmeans function. It creates a confusion matrix and visualizes the clustering results using ggplot.\nThe Wine dataset scales the variables of the wine dataset and performs k-means clustering with three clusters and 25 initial configurations. It plots the total within-cluster sum of squares (wss) to help determine the optimal number of clusters and visualizes the clustering results using fviz_cluster, fviz_nbclust, and fviz_silhouette functions.\nOverall, the code demonstrates various unsupervised learning techniques for clustering analysis, along with visualization methods using ggplot2 and other packages. It applies these techniques to different datasets, allowing for exploration and interpretation of the data’s underlying structure.\n\n\n\n\nlibrary(datasets)\nlibrary(ISLR)\narrest = USArrests\nstates=row.names(USArrests)\nnames(USArrests)\n\n[1] \"Murder\"   \"Assault\"  \"UrbanPop\" \"Rape\"    \n\n# Get means and variances of variables\napply(USArrests, 2, mean)\n\n  Murder  Assault UrbanPop     Rape \n   7.788  170.760   65.540   21.232 \n\napply(USArrests, 2, var)\n\n    Murder    Assault   UrbanPop       Rape \n  18.97047 6945.16571  209.51878   87.72916 \n\n# PCA with scaling\npr.out=prcomp(USArrests, scale=TRUE)\nnames(pr.out) # Five\n\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\npr.out$center # the centering and scaling used (means)\n\n  Murder  Assault UrbanPop     Rape \n   7.788  170.760   65.540   21.232 \n\npr.out$scale # the matrix of variable loadings (eigenvectors)\n\n   Murder   Assault  UrbanPop      Rape \n 4.355510 83.337661 14.474763  9.366385 \n\npr.out$rotation\n\n                PC1        PC2        PC3         PC4\nMurder   -0.5358995 -0.4181809  0.3412327  0.64922780\nAssault  -0.5831836 -0.1879856  0.2681484 -0.74340748\nUrbanPop -0.2781909  0.8728062  0.3780158  0.13387773\nRape     -0.5434321  0.1673186 -0.8177779  0.08902432\n\ndim(pr.out$x)\n\n[1] 50  4\n\npr.out$rotation=-pr.out$rotation\npr.out$x=-pr.out$x\nbiplot(pr.out, scale=0)\n\n\n\npr.out$sdev\n\n[1] 1.5748783 0.9948694 0.5971291 0.4164494\n\npr.var=pr.out$sdev^2\npr.var\n\n[1] 2.4802416 0.9897652 0.3565632 0.1734301\n\npve=pr.var/sum(pr.var)\npve\n\n[1] 0.62006039 0.24744129 0.08914080 0.04335752\n\nplot(pve, xlab=\"Principal Component\", ylab=\"Proportion of Variance Explained\", ylim=c(0,1),type='b')\n\n\n\nplot(cumsum(pve), xlab=\"Principal Component\", ylab=\"Cumulative Proportion of Variance Explained\", ylim=c(0,1),type='b')\n\n\n\n## Use factoextra package\nlibrary(factoextra)\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\nfviz(pr.out, \"ind\", geom = \"auto\", mean.point = TRUE, font.family = \"Georgia\")\n\n\n\nfviz_pca_biplot(pr.out, font.family = \"Georgia\", col.var=\"firebrick1\")\n\n\n\n\nThis R code performs Principal Component Analysis (PCA) on the USArrests dataset and visualizes the results.\nFirst, It calculates the means and variances of each variable in the USArrests dataset using the apply function. Then, it performs PCA on the USArrests dataset with scaling (scale=TRUE) using the prcomp function, storing the results in pr.out.\nIt extracts various components of the PCA results:\n\nNames of the components using names(pr.out).\nCentering and scaling used (means) using pr.out$center.\nMatrix of variable loadings (eigenvectors) using pr.out$rotation.\nStandard deviations of the principal components using pr.out$sdev.\n\nIt also calculates the proportion of variance explained (PVE) by each principal component and plots it and also plots the cumulative proportion of variance explained by the principal components.\nIt visualizes the PCA results using the biplot function to create a biplot showing the principal components and variable loadings and utilizes the factoextra package for additional visualization."
  },
  {
    "objectID": "6323HW4.html#gml_clustering.r",
    "href": "6323HW4.html#gml_clustering.r",
    "title": "Assignment 4: Unsupervised Learning",
    "section": "",
    "text": "library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(RColorBrewer)\n\ncomputers = read.csv(\"https://raw.githubusercontent.com/guru99-edu/R-Programming/master/computers.csv\") \n\n# Only retain two variables for illustration\nrescaled_comp &lt;- computers[4:5] %&gt;%\n  mutate(hd_scal = scale(hd),\n         ram_scal = scale(ram)) %&gt;%\n  select(c(hd_scal, ram_scal))\n        \nggplot(data = rescaled_comp, aes(x = hd_scal, y = ram_scal)) +\n  geom_point(pch=20, col = \"blue\") + theme_bw() +\n  labs(x = \"Hard drive size (Scaled)\", y =\"RAM size (Scaled)\" ) +\n  theme(text = element_text(family=\"Georgia\")) \n\n\n\n# install.packages(\"animation\")\nlibrary(animation)\nset.seed(2345)\nlibrary(animation)\n\n# Animate the K-mean clustering process, cluster no. = 4\nkmeans.ani(rescaled_comp[1:2], centers = 4, pch = 15:18, col = 1:4) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Iris example\n\n# Without grouping by species\nggplot(iris, aes(Petal.Length, Petal.Width)) + geom_point() + \n  theme_bw() +\n  scale_color_manual(values=c(\"firebrick1\",\"forestgreen\",\"darkblue\"))\n\n\n\n# With grouping by species\nggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point() + \n  theme_bw() +\n  scale_color_manual(values=c(\"firebrick1\",\"forestgreen\",\"darkblue\"))\n\n\n\n# Check k-means clusters\n## Starting with three clusters and 20 initial configurations\nset.seed(20)\nirisCluster &lt;- kmeans(iris[, 3:4], 3, nstart = 20)\nirisCluster\n\nK-means clustering with 3 clusters of sizes 52, 48, 50\n\nCluster means:\n  Petal.Length Petal.Width\n1     4.269231    1.342308\n2     5.595833    2.037500\n3     1.462000    0.246000\n\nClustering vector:\n  [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n [38] 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [75] 1 1 1 2 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 2 2 2 2\n[112] 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2\n[149] 2 2\n\nWithin cluster sum of squares by cluster:\n[1] 13.05769 16.29167  2.02200\n (between_SS / total_SS =  94.3 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\nclass(irisCluster$cluster)\n\n[1] \"integer\"\n\n# Confusion matrix\ntable(irisCluster$cluster, iris$Species)\n\n   \n    setosa versicolor virginica\n  1      0         48         4\n  2      0          2        46\n  3     50          0         0\n\nirisCluster$cluster &lt;- as.factor(irisCluster$cluster)\nggplot(iris, aes(Petal.Length, Petal.Width, color = irisCluster$cluster)) + geom_point() +\n  scale_color_manual(values=c(\"firebrick1\",\"forestgreen\",\"darkblue\")) +\n  theme_bw()\n\n\n\nactual = ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point() + \n  theme_bw() +\n  scale_color_manual(values=c(\"firebrick1\",\"forestgreen\",\"darkblue\")) +\n  theme(legend.position=\"bottom\") +\n  theme(text = element_text(family=\"Georgia\")) \nkmc = ggplot(iris, aes(Petal.Length, Petal.Width, color = irisCluster$cluster)) + geom_point() +\n  theme_bw() +\n  scale_color_manual(values=c(\"firebrick1\", \"darkblue\", \"forestgreen\")) +\n  theme(legend.position=\"bottom\") +\n  theme(text = element_text(family=\"Georgia\")) \nlibrary(grid)\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\ngrid.arrange(arrangeGrob(actual, kmc, ncol=2, widths=c(1,1)), nrow=1)\n\n\n\n\nThis R code performs various analyses and visualizations using unsupervised learning techniques, particularly focusing on clustering algorithms such as k-means and hierarchical clustering.\nIt creates a scatter plot using ggplot for the rescaled computer data (rescaled_comp) showing the relationship between hard drive size and RAM size and it animates the k-means clustering process for the rescaled computer data (rescaled_comp) with four clusters using the kmeans.ani function.\nFor the Iris dataset analysis, it creates scatter plots of petal length against petal width for the Iris dataset, both without and with coloring by species. It performs k-means clustering on the Iris dataset (iris) with three clusters and 20 initial configurations using the kmeans function. It creates a confusion matrix and visualizes the clustering results using ggplot.\nThe Wine dataset scales the variables of the wine dataset and performs k-means clustering with three clusters and 25 initial configurations. It plots the total within-cluster sum of squares (wss) to help determine the optimal number of clusters and visualizes the clustering results using fviz_cluster, fviz_nbclust, and fviz_silhouette functions.\nOverall, the code demonstrates various unsupervised learning techniques for clustering analysis, along with visualization methods using ggplot2 and other packages. It applies these techniques to different datasets, allowing for exploration and interpretation of the data’s underlying structure."
  },
  {
    "objectID": "6323HW4.html#gml_pca.r",
    "href": "6323HW4.html#gml_pca.r",
    "title": "Assignment 4: Unsupervised Learning",
    "section": "",
    "text": "library(datasets)\nlibrary(ISLR)\narrest = USArrests\nstates=row.names(USArrests)\nnames(USArrests)\n\n[1] \"Murder\"   \"Assault\"  \"UrbanPop\" \"Rape\"    \n\n# Get means and variances of variables\napply(USArrests, 2, mean)\n\n  Murder  Assault UrbanPop     Rape \n   7.788  170.760   65.540   21.232 \n\napply(USArrests, 2, var)\n\n    Murder    Assault   UrbanPop       Rape \n  18.97047 6945.16571  209.51878   87.72916 \n\n# PCA with scaling\npr.out=prcomp(USArrests, scale=TRUE)\nnames(pr.out) # Five\n\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\npr.out$center # the centering and scaling used (means)\n\n  Murder  Assault UrbanPop     Rape \n   7.788  170.760   65.540   21.232 \n\npr.out$scale # the matrix of variable loadings (eigenvectors)\n\n   Murder   Assault  UrbanPop      Rape \n 4.355510 83.337661 14.474763  9.366385 \n\npr.out$rotation\n\n                PC1        PC2        PC3         PC4\nMurder   -0.5358995 -0.4181809  0.3412327  0.64922780\nAssault  -0.5831836 -0.1879856  0.2681484 -0.74340748\nUrbanPop -0.2781909  0.8728062  0.3780158  0.13387773\nRape     -0.5434321  0.1673186 -0.8177779  0.08902432\n\ndim(pr.out$x)\n\n[1] 50  4\n\npr.out$rotation=-pr.out$rotation\npr.out$x=-pr.out$x\nbiplot(pr.out, scale=0)\n\n\n\npr.out$sdev\n\n[1] 1.5748783 0.9948694 0.5971291 0.4164494\n\npr.var=pr.out$sdev^2\npr.var\n\n[1] 2.4802416 0.9897652 0.3565632 0.1734301\n\npve=pr.var/sum(pr.var)\npve\n\n[1] 0.62006039 0.24744129 0.08914080 0.04335752\n\nplot(pve, xlab=\"Principal Component\", ylab=\"Proportion of Variance Explained\", ylim=c(0,1),type='b')\n\n\n\nplot(cumsum(pve), xlab=\"Principal Component\", ylab=\"Cumulative Proportion of Variance Explained\", ylim=c(0,1),type='b')\n\n\n\n## Use factoextra package\nlibrary(factoextra)\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\nfviz(pr.out, \"ind\", geom = \"auto\", mean.point = TRUE, font.family = \"Georgia\")\n\n\n\nfviz_pca_biplot(pr.out, font.family = \"Georgia\", col.var=\"firebrick1\")\n\n\n\n\nThis R code performs Principal Component Analysis (PCA) on the USArrests dataset and visualizes the results.\nFirst, It calculates the means and variances of each variable in the USArrests dataset using the apply function. Then, it performs PCA on the USArrests dataset with scaling (scale=TRUE) using the prcomp function, storing the results in pr.out.\nIt extracts various components of the PCA results:\n\nNames of the components using names(pr.out).\nCentering and scaling used (means) using pr.out$center.\nMatrix of variable loadings (eigenvectors) using pr.out$rotation.\nStandard deviations of the principal components using pr.out$sdev.\n\nIt also calculates the proportion of variance explained (PVE) by each principal component and plots it and also plots the cumulative proportion of variance explained by the principal components.\nIt visualizes the PCA results using the biplot function to create a biplot showing the principal components and variable loadings and utilizes the factoextra package for additional visualization."
  },
  {
    "objectID": "6323HW5.html",
    "href": "6323HW5.html",
    "title": "Assignment 4 & 5: Unsupervised Learning",
    "section": "",
    "text": "Unsupervised learning is a class of machine learning algorithms to identify patterns or grouping structure in the data. Unlike supervised learning which relies on “supervised” information such as the dependent variable to guide modeling, unsupervised learning seeks to explore the structure and possible groupings of unlabeled data. This information will be useful to provide pre-processor for supervised learning.\nUnsupervised learning has no explicit dependent variable of Y for prediction. Instead, the goal is to discover interesting patterns about the measurements on \\((X_{1}), (X_{2}), . . . , (X_{p})\\) and identify any subgroups among the observations.\nGenerally, in this section, the two general methods are introduced: Principal components analysis and Clustering.\n\n\nPrincipal Components Analysis (PCA) produces a low-dimensional representation of a dataset. It finds a sequence of linear combinations of the variables that have maximal variance, and are mutually uncorrelated.\nThe first principal component of a set of features \\((X_1, X_2, . . . , X_p)\\) is the normalized linear combination of the features:  \\[  Z_1 = \\phi_{11}X_1 +\\phi_{21}X_2 +...+\\phi_{p1}X_p \\] \nthat has the largest variance. By normalized, we mean that \\(\\sum_{j=1}^p\\phi_{j1}^2 = 1\\).\nThe elements \\((\\phi_{11}, . . . , \\phi_{p1})\\) are the loadings of the first principal component; together, the loadings make up the principal component loading vector, \\(\\phi_1= (\\phi_{11} \\phi_{21} ... \\phi_{p1})^T\\)\nWe constrain the loadings so that their sum of squares is equal to one, since otherwise setting these elements to be arbitrarily large in absolute value could result in an arbitrarily large variance.\n\n\n\n\n\nThe K-means clustering method is to partition the data points into k groups such that the sum of squares from points to the assigned cluster center in each group is minimized.\n\n\n\nHierarchical clustering is an alternative approach which does not require a pre-specified or a particular choice of \\((K)\\).\nHierarchical Clustering has an advantage that it produces a tree-based representation of the observations: Dendrogram\nA dendrogram is built starting from the leaves and combining clusters up to the trunk. The result of hierarchical clustering is a tree-based representation of the objects, which is also known as dendrogram. Observations can be subdivided into groups by cutting the dendrogram at a desired similarity level."
  },
  {
    "objectID": "6323HW5.html#principal-component-analysis-pca",
    "href": "6323HW5.html#principal-component-analysis-pca",
    "title": "Assignment 4 & 5: Unsupervised Learning",
    "section": "",
    "text": "Principal Components Analysis (PCA) produces a low-dimensional representation of a dataset. It finds a sequence of linear combinations of the variables that have maximal variance, and are mutually uncorrelated.\nThe first principal component of a set of features \\((X_1, X_2, . . . , X_p)\\) is the normalized linear combination of the features:  \\[  Z_1 = \\phi_{11}X_1 +\\phi_{21}X_2 +...+\\phi_{p1}X_p \\] \nthat has the largest variance. By normalized, we mean that \\(\\sum_{j=1}^p\\phi_{j1}^2 = 1\\).\nThe elements \\((\\phi_{11}, . . . , \\phi_{p1})\\) are the loadings of the first principal component; together, the loadings make up the principal component loading vector, \\(\\phi_1= (\\phi_{11} \\phi_{21} ... \\phi_{p1})^T\\)\nWe constrain the loadings so that their sum of squares is equal to one, since otherwise setting these elements to be arbitrarily large in absolute value could result in an arbitrarily large variance."
  },
  {
    "objectID": "6323HW5.html#clustering",
    "href": "6323HW5.html#clustering",
    "title": "Assignment 4 & 5: Unsupervised Learning",
    "section": "",
    "text": "The K-means clustering method is to partition the data points into k groups such that the sum of squares from points to the assigned cluster center in each group is minimized.\n\n\n\nHierarchical clustering is an alternative approach which does not require a pre-specified or a particular choice of \\((K)\\).\nHierarchical Clustering has an advantage that it produces a tree-based representation of the observations: Dendrogram\nA dendrogram is built starting from the leaves and combining clusters up to the trunk. The result of hierarchical clustering is a tree-based representation of the objects, which is also known as dendrogram. Observations can be subdivided into groups by cutting the dendrogram at a desired similarity level."
  },
  {
    "objectID": "6323HW6.html",
    "href": "6323HW6.html",
    "title": "Assignment 6: Linear Regression",
    "section": "",
    "text": "library(easypackages)\nlibraries(\"arm\",\"MASS\",\"ISLR\")\n\nLoading required package: arm\n\n\nLoading required package: MASS\n\n\nLoading required package: Matrix\n\n\nLoading required package: lme4\n\n\n\narm (Version 1.14-4, built: 2024-4-1)\n\n\nWorking directory is /Users/jhanvi/Downloads/jhanvi-kannan.github.io\n\n\nLoading required package: ISLR\n\n\nAll packages loaded successfully\n\n## Load datasets from MASS and ISLR packages\nattach(Boston)\n\n### Simple linear regression\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# What is the Boston dataset?\n?Boston\nplot(medv~lstat,Boston, pch=20, cex=.8, col=\"steelblue\")\nfit1=lm(medv~lstat,data=Boston)\nfit1\n\n\nCall:\nlm(formula = medv ~ lstat, data = Boston)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(fit1)\n\n\nCall:\nlm(formula = medv ~ lstat, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nabline(fit1,col=\"firebrick\")\n\n\n\nnames(fit1)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\nconfint(fit1) # confidence intervals\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\n# Predictions using values in lstat\npredict(fit1,data.frame(lstat=c(0,5,10,15)),interval=\"confidence\") # confidence intervals\n\n       fit      lwr      upr\n1 34.55384 33.44846 35.65922\n2 29.80359 29.00741 30.59978\n3 25.05335 24.47413 25.63256\n4 20.30310 19.73159 20.87461\n\npredict(fit1,data.frame(lstat=c(0,5,10,15)),interval=\"prediction\") # prediction intervals\n\n       fit       lwr      upr\n1 34.55384 22.291923 46.81576\n2 29.80359 17.565675 42.04151\n3 25.05335 12.827626 37.27907\n4 20.30310  8.077742 32.52846\n\n# Prediction interval uses sample mean and takes into account the variability of the estimators for μ and σ.\n# Therefore, the interval will be wider.\n\n### Multiple linear regression\nfit2=lm(medv~lstat+age,data=Boston)\nsummary(fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nfit3=lm(medv~.,Boston)\nsummary(fit3)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\npar(mfrow=c(2,2))\nplot(fit3,pch=20, cex=.8, col=\"steelblue\")\nmtext(\"fit3\", side = 3, line = - 2, cex = 2, outer = TRUE)\n\n\n\n# Update function to re-specify the model, i.e. include all but age and indus variables\nfit4=update(fit3,~.-age-indus)\nsummary(fit4)\n\n\nCall:\nlm(formula = medv ~ crim + zn + chas + nox + rm + dis + rad + \n    tax + ptratio + black + lstat, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.5984  -2.7386  -0.5046   1.7273  26.2373 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.341145   5.067492   7.171 2.73e-12 ***\ncrim         -0.108413   0.032779  -3.307 0.001010 ** \nzn            0.045845   0.013523   3.390 0.000754 ***\nchas          2.718716   0.854240   3.183 0.001551 ** \nnox         -17.376023   3.535243  -4.915 1.21e-06 ***\nrm            3.801579   0.406316   9.356  &lt; 2e-16 ***\ndis          -1.492711   0.185731  -8.037 6.84e-15 ***\nrad           0.299608   0.063402   4.726 3.00e-06 ***\ntax          -0.011778   0.003372  -3.493 0.000521 ***\nptratio      -0.946525   0.129066  -7.334 9.24e-13 ***\nblack         0.009291   0.002674   3.475 0.000557 ***\nlstat        -0.522553   0.047424 -11.019  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.736 on 494 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7348 \nF-statistic: 128.2 on 11 and 494 DF,  p-value: &lt; 2.2e-16\n\n# Set the next plot configuration\npar(mfrow=c(2,2), main=\"fit4\")\n\nWarning in par(mfrow = c(2, 2), main = \"fit4\"): \"main\" is not a graphical\nparameter\n\nplot(fit4,pch=20, cex=.8, col=\"steelblue\")\nmtext(\"fit4\", side = 3, line = - 2, cex = 2, outer = TRUE)\n\n\n\n# Uses coefplot to plot coefficients.  Note the line at 0.\npar(mfrow=c(1,1))\narm::coefplot(fit4)\n\n\n\n### Nonlinear terms and Interactions\nfit5=lm(medv~lstat*age,Boston) # include both variables and the interaction term x1:x2\nsummary(fit5)\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16\n\n## I() identity function for squared term to interpret as-is\n## Combine two command lines with semicolon\nfit6=lm(medv~lstat +I(lstat^2),Boston); summary(fit6)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2), data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\npar(mfrow=c(1,1))\nplot(medv~lstat, pch=20, col=\"forestgreen\")\n\npoints(lstat,fitted(fit6),col=\"firebrick\",pch=20)\nfit7=lm(medv~poly(lstat,4))\npoints(lstat,fitted(fit7),col=\"steelblue\",pch=20)\n\n\n\n###Qualitative predictors\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nsummary(Carseats)\n\n     Sales          CompPrice       Income        Advertising    \n Min.   : 0.000   Min.   : 77   Min.   : 21.00   Min.   : 0.000  \n 1st Qu.: 5.390   1st Qu.:115   1st Qu.: 42.75   1st Qu.: 0.000  \n Median : 7.490   Median :125   Median : 69.00   Median : 5.000  \n Mean   : 7.496   Mean   :125   Mean   : 68.66   Mean   : 6.635  \n 3rd Qu.: 9.320   3rd Qu.:135   3rd Qu.: 91.00   3rd Qu.:12.000  \n Max.   :16.270   Max.   :175   Max.   :120.00   Max.   :29.000  \n   Population        Price        ShelveLoc        Age          Education   \n Min.   : 10.0   Min.   : 24.0   Bad   : 96   Min.   :25.00   Min.   :10.0  \n 1st Qu.:139.0   1st Qu.:100.0   Good  : 85   1st Qu.:39.75   1st Qu.:12.0  \n Median :272.0   Median :117.0   Medium:219   Median :54.50   Median :14.0  \n Mean   :264.8   Mean   :115.8                Mean   :53.32   Mean   :13.9  \n 3rd Qu.:398.5   3rd Qu.:131.0                3rd Qu.:66.00   3rd Qu.:16.0  \n Max.   :509.0   Max.   :191.0                Max.   :80.00   Max.   :18.0  \n Urban       US     \n No :118   No :142  \n Yes:282   Yes:258  \n                    \n                    \n                    \n                    \n\nfit1=lm(Sales~.+Income:Advertising+Age:Price,Carseats) # add two interaction terms\nsummary(fit1)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Age:Price, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(Carseats$ShelveLoc) # what is contrasts function?\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n?contrasts\n\n### Writing an R function to combine the lm, plot and abline functions to \n### create a one step regression fit plot function\nregplot=function(x,y){\n  fit=lm(y~x)\n  plot(x,y, pch=20)\n  abline(fit,col=\"firebrick\")\n}\nattach(Carseats)\n\nThe following objects are masked from Carseats (pos = 3):\n\n    Advertising, Age, CompPrice, Education, Income, Population, Price,\n    Sales, ShelveLoc, Urban, US\n\nregplot(Price,Sales)\n\n\n\n## Allow extra room for additional arguments/specifications\nregplot=function(x,y,...){\n  fit=lm(y~x)\n  plot(x,y,...)\n  abline(fit,col=\"firebrick\")\n}\nregplot(Price,Sales,xlab=\"Price\",ylab=\"Sales\",col=\"steelblue\",pch=20)\n\n\n\n\nThis R script performs a simple linear regression of medv (median house value) on lstat (percent of lower status population) using the lm() function. It plots the data points and the fitted regression line using plot() and abline() functions and confidence intervals and prediction intervals are computed using the predict() function.\nIt next performs multiple linear regression using the lm() function with more than one predictor variable. Various summary statistics and diagnostic plots (e.g., residual plots) are generated to evaluate the model. Model fit and plots are generated to visualize the relationship between predictors and the response variable.\n\n\n\n\n# Load necessary packages\nlibrary(haven)\n\n# Read the TEDS2016 dataset\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\n# Run logistic regression model\nglm.vt &lt;- glm(votetsai ~ female, data = TEDS_2016, family = binomial)\n\n# Summary and diagnostics\nsummary(glm.vt)\n\n\nCall:\nglm(formula = votetsai ~ female, family = binomial, data = TEDS_2016)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  0.54971    0.08245   6.667 2.61e-11 ***\nfemale      -0.06517    0.11644  -0.560    0.576    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1666.5  on 1260  degrees of freedom\nResidual deviance: 1666.2  on 1259  degrees of freedom\n  (429 observations deleted due to missingness)\nAIC: 1670.2\n\nNumber of Fisher Scoring iterations: 4\n\nplot(glm.vt)\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on the logistic regression model results:\n\nThe coefficient for the predictor variable female is estimated to be -0.06517 with a standard error of 0.11644.\nThe p-value associated with the female predictor is 0.576, which is greater than the typical significance level of 0.05.\nThe confidence interval for the coefficient of female includes zero, ranging from -0.294 to 0.163.\n\nThese results indicate that there is no statistically significant association between being a female voter and voting for President Tsai Ing-wen. The coefficient for the female predictor is negative, but its p-value is not significant, suggesting that gender (female) does not have a significant effect on the likelihood of voting for President Tsai.\nTherefore, based on this logistic regression model, we cannot conclude that female voters are more or less likely to vote for President Tsai compared to male voters. Other factors not included in this model may better explain the voting behavior.\nAs for diagnostics, the summary output provides the null and residual deviances, which can be used to assess the goodness of fit of the model. In this case, the residual deviance is slightly smaller than the null deviance, suggesting that the model fits the data reasonably well. Additionally, the AIC (Akaike Information Criterion) value is provided, which can be used for model comparison purposes. However, since we only have one predictor variable in this model, there isn’t much to diagnose beyond the significance of the coefficient.\n\n\n\n# Update the model formula to include additional variables\nglm.vt &lt;- glm(votetsai ~ female + KMT + DPP + age + edu + income, data = TEDS_2016, family = binomial)\n\n# Summary and diagnostics\nsummary(glm.vt)\n\n\nCall:\nglm(formula = votetsai ~ female + KMT + DPP + age + edu + income, \n    family = binomial, data = TEDS_2016)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.618640   0.592084   2.734  0.00626 ** \nfemale       0.047406   0.177403   0.267  0.78930    \nKMT         -3.156273   0.250360 -12.607  &lt; 2e-16 ***\nDPP          2.888943   0.267968  10.781  &lt; 2e-16 ***\nage         -0.011808   0.007164  -1.648  0.09931 .  \nedu         -0.184604   0.083102  -2.221  0.02632 *  \nincome       0.013727   0.034382   0.399  0.68971    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1661.76  on 1256  degrees of freedom\nResidual deviance:  836.15  on 1250  degrees of freedom\n  (433 observations deleted due to missingness)\nAIC: 850.15\n\nNumber of Fisher Scoring iterations: 6\n\nplot(glm.vt)\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the extended logistic regression model, the coefficients and their significance levels provide insights into the relationship between the predictors and the likelihood of voting for President Tsai Ing-wen (votetsai).\nIn terms of explaining/predicting voting behavior (votetsai), the group of variables including party affiliation (KMT and DPP) appears to be the most influential, as indicated by their strong statistical significance. Other demographic variables such as age and edu also contribute to the model but to a lesser extent. The inclusion of female and income did not significantly improve the model’s explanatory power.\nOverall, party affiliation and certain demographic characteristics seem to be better predictors of voting behavior for President Tsai Ing-wen compared to gender and income. However, it’s essential to consider the context and potential confounding factors when interpreting these results.\n\n\n\n\n# Update the model formula to include additional variables\nglm.vt &lt;- glm(votetsai ~ female + Independence + Econ_worse + Govt_dont_care + Minnan_father + Mainland_father + Taiwanese + KMT + DPP + age + edu + income, data = TEDS_2016, family = binomial)\n\n# Summary and diagnostics\nsummary(glm.vt)\n\n\nCall:\nglm(formula = votetsai ~ female + Independence + Econ_worse + \n    Govt_dont_care + Minnan_father + Mainland_father + Taiwanese + \n    KMT + DPP + age + edu + income, family = binomial, data = TEDS_2016)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.015976   0.679780  -0.024  0.98125    \nfemale          -0.097996   0.189840  -0.516  0.60571    \nIndependence     1.020953   0.251776   4.055 5.01e-05 ***\nEcon_worse       0.310462   0.189100   1.642  0.10063    \nGovt_dont_care  -0.014295   0.188765  -0.076  0.93964    \nMinnan_father   -0.247650   0.253921  -0.975  0.32941    \nMainland_father -1.089332   0.396822  -2.745  0.00605 ** \nTaiwanese        0.909019   0.198930   4.570 4.89e-06 ***\nKMT             -2.922246   0.259333 -11.268  &lt; 2e-16 ***\nDPP              2.468855   0.275350   8.966  &lt; 2e-16 ***\nage              0.003287   0.007884   0.417  0.67672    \nedu             -0.092110   0.090119  -1.022  0.30674    \nincome           0.021771   0.036406   0.598  0.54984    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1661.76  on 1256  degrees of freedom\nResidual deviance:  767.13  on 1244  degrees of freedom\n  (433 observations deleted due to missingness)\nAIC: 793.13\n\nNumber of Fisher Scoring iterations: 6\n\nplot(glm.vt)\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on this logistic regression model, gender (female) does not appear to be a significant predictor of voting for President Tsai Ing-wen, while party affiliation (KMT and DPP), age, and education level (edu) are significant predictors. However, it’s essential to interpret these results in the context of the data and the assumptions of the model."
  },
  {
    "objectID": "6323HW6.html#lab_linearregression01.r",
    "href": "6323HW6.html#lab_linearregression01.r",
    "title": "Assignment 6: Linear Regression",
    "section": "",
    "text": "library(easypackages)\nlibraries(\"arm\",\"MASS\",\"ISLR\")\n\nLoading required package: arm\n\n\nLoading required package: MASS\n\n\nLoading required package: Matrix\n\n\nLoading required package: lme4\n\n\n\narm (Version 1.14-4, built: 2024-4-1)\n\n\nWorking directory is /Users/jhanvi/Downloads/jhanvi-kannan.github.io\n\n\nLoading required package: ISLR\n\n\nAll packages loaded successfully\n\n## Load datasets from MASS and ISLR packages\nattach(Boston)\n\n### Simple linear regression\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# What is the Boston dataset?\n?Boston\nplot(medv~lstat,Boston, pch=20, cex=.8, col=\"steelblue\")\nfit1=lm(medv~lstat,data=Boston)\nfit1\n\n\nCall:\nlm(formula = medv ~ lstat, data = Boston)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(fit1)\n\n\nCall:\nlm(formula = medv ~ lstat, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nabline(fit1,col=\"firebrick\")\n\n\n\nnames(fit1)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\nconfint(fit1) # confidence intervals\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\n# Predictions using values in lstat\npredict(fit1,data.frame(lstat=c(0,5,10,15)),interval=\"confidence\") # confidence intervals\n\n       fit      lwr      upr\n1 34.55384 33.44846 35.65922\n2 29.80359 29.00741 30.59978\n3 25.05335 24.47413 25.63256\n4 20.30310 19.73159 20.87461\n\npredict(fit1,data.frame(lstat=c(0,5,10,15)),interval=\"prediction\") # prediction intervals\n\n       fit       lwr      upr\n1 34.55384 22.291923 46.81576\n2 29.80359 17.565675 42.04151\n3 25.05335 12.827626 37.27907\n4 20.30310  8.077742 32.52846\n\n# Prediction interval uses sample mean and takes into account the variability of the estimators for μ and σ.\n# Therefore, the interval will be wider.\n\n### Multiple linear regression\nfit2=lm(medv~lstat+age,data=Boston)\nsummary(fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nfit3=lm(medv~.,Boston)\nsummary(fit3)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\npar(mfrow=c(2,2))\nplot(fit3,pch=20, cex=.8, col=\"steelblue\")\nmtext(\"fit3\", side = 3, line = - 2, cex = 2, outer = TRUE)\n\n\n\n# Update function to re-specify the model, i.e. include all but age and indus variables\nfit4=update(fit3,~.-age-indus)\nsummary(fit4)\n\n\nCall:\nlm(formula = medv ~ crim + zn + chas + nox + rm + dis + rad + \n    tax + ptratio + black + lstat, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.5984  -2.7386  -0.5046   1.7273  26.2373 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.341145   5.067492   7.171 2.73e-12 ***\ncrim         -0.108413   0.032779  -3.307 0.001010 ** \nzn            0.045845   0.013523   3.390 0.000754 ***\nchas          2.718716   0.854240   3.183 0.001551 ** \nnox         -17.376023   3.535243  -4.915 1.21e-06 ***\nrm            3.801579   0.406316   9.356  &lt; 2e-16 ***\ndis          -1.492711   0.185731  -8.037 6.84e-15 ***\nrad           0.299608   0.063402   4.726 3.00e-06 ***\ntax          -0.011778   0.003372  -3.493 0.000521 ***\nptratio      -0.946525   0.129066  -7.334 9.24e-13 ***\nblack         0.009291   0.002674   3.475 0.000557 ***\nlstat        -0.522553   0.047424 -11.019  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.736 on 494 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7348 \nF-statistic: 128.2 on 11 and 494 DF,  p-value: &lt; 2.2e-16\n\n# Set the next plot configuration\npar(mfrow=c(2,2), main=\"fit4\")\n\nWarning in par(mfrow = c(2, 2), main = \"fit4\"): \"main\" is not a graphical\nparameter\n\nplot(fit4,pch=20, cex=.8, col=\"steelblue\")\nmtext(\"fit4\", side = 3, line = - 2, cex = 2, outer = TRUE)\n\n\n\n# Uses coefplot to plot coefficients.  Note the line at 0.\npar(mfrow=c(1,1))\narm::coefplot(fit4)\n\n\n\n### Nonlinear terms and Interactions\nfit5=lm(medv~lstat*age,Boston) # include both variables and the interaction term x1:x2\nsummary(fit5)\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16\n\n## I() identity function for squared term to interpret as-is\n## Combine two command lines with semicolon\nfit6=lm(medv~lstat +I(lstat^2),Boston); summary(fit6)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2), data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\npar(mfrow=c(1,1))\nplot(medv~lstat, pch=20, col=\"forestgreen\")\n\npoints(lstat,fitted(fit6),col=\"firebrick\",pch=20)\nfit7=lm(medv~poly(lstat,4))\npoints(lstat,fitted(fit7),col=\"steelblue\",pch=20)\n\n\n\n###Qualitative predictors\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nsummary(Carseats)\n\n     Sales          CompPrice       Income        Advertising    \n Min.   : 0.000   Min.   : 77   Min.   : 21.00   Min.   : 0.000  \n 1st Qu.: 5.390   1st Qu.:115   1st Qu.: 42.75   1st Qu.: 0.000  \n Median : 7.490   Median :125   Median : 69.00   Median : 5.000  \n Mean   : 7.496   Mean   :125   Mean   : 68.66   Mean   : 6.635  \n 3rd Qu.: 9.320   3rd Qu.:135   3rd Qu.: 91.00   3rd Qu.:12.000  \n Max.   :16.270   Max.   :175   Max.   :120.00   Max.   :29.000  \n   Population        Price        ShelveLoc        Age          Education   \n Min.   : 10.0   Min.   : 24.0   Bad   : 96   Min.   :25.00   Min.   :10.0  \n 1st Qu.:139.0   1st Qu.:100.0   Good  : 85   1st Qu.:39.75   1st Qu.:12.0  \n Median :272.0   Median :117.0   Medium:219   Median :54.50   Median :14.0  \n Mean   :264.8   Mean   :115.8                Mean   :53.32   Mean   :13.9  \n 3rd Qu.:398.5   3rd Qu.:131.0                3rd Qu.:66.00   3rd Qu.:16.0  \n Max.   :509.0   Max.   :191.0                Max.   :80.00   Max.   :18.0  \n Urban       US     \n No :118   No :142  \n Yes:282   Yes:258  \n                    \n                    \n                    \n                    \n\nfit1=lm(Sales~.+Income:Advertising+Age:Price,Carseats) # add two interaction terms\nsummary(fit1)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Age:Price, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(Carseats$ShelveLoc) # what is contrasts function?\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n?contrasts\n\n### Writing an R function to combine the lm, plot and abline functions to \n### create a one step regression fit plot function\nregplot=function(x,y){\n  fit=lm(y~x)\n  plot(x,y, pch=20)\n  abline(fit,col=\"firebrick\")\n}\nattach(Carseats)\n\nThe following objects are masked from Carseats (pos = 3):\n\n    Advertising, Age, CompPrice, Education, Income, Population, Price,\n    Sales, ShelveLoc, Urban, US\n\nregplot(Price,Sales)\n\n\n\n## Allow extra room for additional arguments/specifications\nregplot=function(x,y,...){\n  fit=lm(y~x)\n  plot(x,y,...)\n  abline(fit,col=\"firebrick\")\n}\nregplot(Price,Sales,xlab=\"Price\",ylab=\"Sales\",col=\"steelblue\",pch=20)\n\n\n\n\nThis R script performs a simple linear regression of medv (median house value) on lstat (percent of lower status population) using the lm() function. It plots the data points and the fitted regression line using plot() and abline() functions and confidence intervals and prediction intervals are computed using the predict() function.\nIt next performs multiple linear regression using the lm() function with more than one predictor variable. Various summary statistics and diagnostic plots (e.g., residual plots) are generated to evaluate the model. Model fit and plots are generated to visualize the relationship between predictors and the response variable."
  },
  {
    "objectID": "6323HW6.html#teds2016-logit-regression",
    "href": "6323HW6.html#teds2016-logit-regression",
    "title": "Assignment 6: Linear Regression",
    "section": "",
    "text": "# Load necessary packages\nlibrary(haven)\n\n# Read the TEDS2016 dataset\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\n# Run logistic regression model\nglm.vt &lt;- glm(votetsai ~ female, data = TEDS_2016, family = binomial)\n\n# Summary and diagnostics\nsummary(glm.vt)\n\n\nCall:\nglm(formula = votetsai ~ female, family = binomial, data = TEDS_2016)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  0.54971    0.08245   6.667 2.61e-11 ***\nfemale      -0.06517    0.11644  -0.560    0.576    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1666.5  on 1260  degrees of freedom\nResidual deviance: 1666.2  on 1259  degrees of freedom\n  (429 observations deleted due to missingness)\nAIC: 1670.2\n\nNumber of Fisher Scoring iterations: 4\n\nplot(glm.vt)\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on the logistic regression model results:\n\nThe coefficient for the predictor variable female is estimated to be -0.06517 with a standard error of 0.11644.\nThe p-value associated with the female predictor is 0.576, which is greater than the typical significance level of 0.05.\nThe confidence interval for the coefficient of female includes zero, ranging from -0.294 to 0.163.\n\nThese results indicate that there is no statistically significant association between being a female voter and voting for President Tsai Ing-wen. The coefficient for the female predictor is negative, but its p-value is not significant, suggesting that gender (female) does not have a significant effect on the likelihood of voting for President Tsai.\nTherefore, based on this logistic regression model, we cannot conclude that female voters are more or less likely to vote for President Tsai compared to male voters. Other factors not included in this model may better explain the voting behavior.\nAs for diagnostics, the summary output provides the null and residual deviances, which can be used to assess the goodness of fit of the model. In this case, the residual deviance is slightly smaller than the null deviance, suggesting that the model fits the data reasonably well. Additionally, the AIC (Akaike Information Criterion) value is provided, which can be used for model comparison purposes. However, since we only have one predictor variable in this model, there isn’t much to diagnose beyond the significance of the coefficient.\n\n\n\n# Update the model formula to include additional variables\nglm.vt &lt;- glm(votetsai ~ female + KMT + DPP + age + edu + income, data = TEDS_2016, family = binomial)\n\n# Summary and diagnostics\nsummary(glm.vt)\n\n\nCall:\nglm(formula = votetsai ~ female + KMT + DPP + age + edu + income, \n    family = binomial, data = TEDS_2016)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.618640   0.592084   2.734  0.00626 ** \nfemale       0.047406   0.177403   0.267  0.78930    \nKMT         -3.156273   0.250360 -12.607  &lt; 2e-16 ***\nDPP          2.888943   0.267968  10.781  &lt; 2e-16 ***\nage         -0.011808   0.007164  -1.648  0.09931 .  \nedu         -0.184604   0.083102  -2.221  0.02632 *  \nincome       0.013727   0.034382   0.399  0.68971    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1661.76  on 1256  degrees of freedom\nResidual deviance:  836.15  on 1250  degrees of freedom\n  (433 observations deleted due to missingness)\nAIC: 850.15\n\nNumber of Fisher Scoring iterations: 6\n\nplot(glm.vt)\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the extended logistic regression model, the coefficients and their significance levels provide insights into the relationship between the predictors and the likelihood of voting for President Tsai Ing-wen (votetsai).\nIn terms of explaining/predicting voting behavior (votetsai), the group of variables including party affiliation (KMT and DPP) appears to be the most influential, as indicated by their strong statistical significance. Other demographic variables such as age and edu also contribute to the model but to a lesser extent. The inclusion of female and income did not significantly improve the model’s explanatory power.\nOverall, party affiliation and certain demographic characteristics seem to be better predictors of voting behavior for President Tsai Ing-wen compared to gender and income. However, it’s essential to consider the context and potential confounding factors when interpreting these results.\n\n\n\n\n# Update the model formula to include additional variables\nglm.vt &lt;- glm(votetsai ~ female + Independence + Econ_worse + Govt_dont_care + Minnan_father + Mainland_father + Taiwanese + KMT + DPP + age + edu + income, data = TEDS_2016, family = binomial)\n\n# Summary and diagnostics\nsummary(glm.vt)\n\n\nCall:\nglm(formula = votetsai ~ female + Independence + Econ_worse + \n    Govt_dont_care + Minnan_father + Mainland_father + Taiwanese + \n    KMT + DPP + age + edu + income, family = binomial, data = TEDS_2016)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.015976   0.679780  -0.024  0.98125    \nfemale          -0.097996   0.189840  -0.516  0.60571    \nIndependence     1.020953   0.251776   4.055 5.01e-05 ***\nEcon_worse       0.310462   0.189100   1.642  0.10063    \nGovt_dont_care  -0.014295   0.188765  -0.076  0.93964    \nMinnan_father   -0.247650   0.253921  -0.975  0.32941    \nMainland_father -1.089332   0.396822  -2.745  0.00605 ** \nTaiwanese        0.909019   0.198930   4.570 4.89e-06 ***\nKMT             -2.922246   0.259333 -11.268  &lt; 2e-16 ***\nDPP              2.468855   0.275350   8.966  &lt; 2e-16 ***\nage              0.003287   0.007884   0.417  0.67672    \nedu             -0.092110   0.090119  -1.022  0.30674    \nincome           0.021771   0.036406   0.598  0.54984    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1661.76  on 1256  degrees of freedom\nResidual deviance:  767.13  on 1244  degrees of freedom\n  (433 observations deleted due to missingness)\nAIC: 793.13\n\nNumber of Fisher Scoring iterations: 6\n\nplot(glm.vt)\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on this logistic regression model, gender (female) does not appear to be a significant predictor of voting for President Tsai Ing-wen, while party affiliation (KMT and DPP), age, and education level (edu) are significant predictors. However, it’s essential to interpret these results in the context of the data and the assumptions of the model."
  },
  {
    "objectID": "6323HW7.html",
    "href": "6323HW7.html",
    "title": "Assignment 7: Logistic Regression",
    "section": "",
    "text": "require(ISLR)\n\nLoading required package: ISLR\n\n# Check dataset Smarket\n?Smarket\nnames(Smarket)\n\n[1] \"Year\"      \"Lag1\"      \"Lag2\"      \"Lag3\"      \"Lag4\"      \"Lag5\"     \n[7] \"Volume\"    \"Today\"     \"Direction\"\n\nsummary(Smarket)\n\n      Year           Lag1                Lag2                Lag3          \n Min.   :2001   Min.   :-4.922000   Min.   :-4.922000   Min.   :-4.922000  \n 1st Qu.:2002   1st Qu.:-0.639500   1st Qu.:-0.639500   1st Qu.:-0.640000  \n Median :2003   Median : 0.039000   Median : 0.039000   Median : 0.038500  \n Mean   :2003   Mean   : 0.003834   Mean   : 0.003919   Mean   : 0.001716  \n 3rd Qu.:2004   3rd Qu.: 0.596750   3rd Qu.: 0.596750   3rd Qu.: 0.596750  \n Max.   :2005   Max.   : 5.733000   Max.   : 5.733000   Max.   : 5.733000  \n      Lag4                Lag5              Volume           Today          \n Min.   :-4.922000   Min.   :-4.92200   Min.   :0.3561   Min.   :-4.922000  \n 1st Qu.:-0.640000   1st Qu.:-0.64000   1st Qu.:1.2574   1st Qu.:-0.639500  \n Median : 0.038500   Median : 0.03850   Median :1.4229   Median : 0.038500  \n Mean   : 0.001636   Mean   : 0.00561   Mean   :1.4783   Mean   : 0.003138  \n 3rd Qu.: 0.596750   3rd Qu.: 0.59700   3rd Qu.:1.6417   3rd Qu.: 0.596750  \n Max.   : 5.733000   Max.   : 5.73300   Max.   :3.1525   Max.   : 5.733000  \n Direction \n Down:602  \n Up  :648  \n           \n           \n           \n           \n\n# Create a dataframe for data browsing\nsm=Smarket\n\n# Bivariate Plot of inter-lag correlations\npairs(Smarket,col=Smarket$Direction,cex=.5, pch=20)\n\n\n\n# Logistic regression\nglm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,\n            data=Smarket,family=binomial)\nsummary(glm.fit)\n\n\nCall:\nglm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + \n    Volume, family = binomial, data = Smarket)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept) -0.126000   0.240736  -0.523    0.601\nLag1        -0.073074   0.050167  -1.457    0.145\nLag2        -0.042301   0.050086  -0.845    0.398\nLag3         0.011085   0.049939   0.222    0.824\nLag4         0.009359   0.049974   0.187    0.851\nLag5         0.010313   0.049511   0.208    0.835\nVolume       0.135441   0.158360   0.855    0.392\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1731.2  on 1249  degrees of freedom\nResidual deviance: 1727.6  on 1243  degrees of freedom\nAIC: 1741.6\n\nNumber of Fisher Scoring iterations: 3\n\nglm.probs=predict(glm.fit,type=\"response\") \nglm.probs[1:5]\n\n        1         2         3         4         5 \n0.5070841 0.4814679 0.4811388 0.5152224 0.5107812 \n\nglm.pred=ifelse(glm.probs&gt;0.5,\"Up\",\"Down\")\nattach(Smarket)\ntable(glm.pred,Direction)\n\n        Direction\nglm.pred Down  Up\n    Down  145 141\n    Up    457 507\n\nmean(glm.pred==Direction)\n\n[1] 0.5216\n\n# Make training and test set for prediction\ntrain = Year&lt;2005\nglm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,\n            data=Smarket,family=binomial, subset=train)\nglm.probs=predict(glm.fit,newdata=Smarket[!train,],type=\"response\") \nglm.pred=ifelse(glm.probs &gt;0.5,\"Up\",\"Down\")\nDirection.2005=Smarket$Direction[!train]\ntable(glm.pred,Direction.2005)\n\n        Direction.2005\nglm.pred Down Up\n    Down   77 97\n    Up     34 44\n\nmean(glm.pred==Direction.2005)\n\n[1] 0.4801587\n\n#Fit smaller model\nglm.fit=glm(Direction~Lag1+Lag2,\n            data=Smarket,family=binomial, subset=train)\nglm.probs=predict(glm.fit,newdata=Smarket[!train,],type=\"response\") \nglm.pred=ifelse(glm.probs &gt;0.5,\"Up\",\"Down\")\ntable(glm.pred,Direction.2005)\n\n        Direction.2005\nglm.pred Down  Up\n    Down   35  35\n    Up     76 106\n\nmean(glm.pred==Direction.2005)\n\n[1] 0.5595238\n\n# Check accuracy rate\n106/(76+106)\n\n[1] 0.5824176\n\n\nThis R script analyzes the “Smarket” dataset, which contains daily percentage returns for the S&P 500 stock index from 2001 to 2005, along with five predictors: “Lag1” to “Lag5” representing the previous day’s percentage returns for the S&P 500 stock index, and “Volume” representing the number of shares traded on the previous day.\n\nThe script creates a bivariate plot to visualize the inter-lag correlations. It uses the pairs() function, with colors determined by the “Direction” variable (whether the market moved up or down).\nThe script also performs logistic regression to predict the direction of the stock market movement (“Up” or “Down”) based on lagged predictors (“Lag1” to “Lag5”) and volume (“Volume”).\nThe logistic regression model’s coefficients and their significance levels are provided in the output of summary(glm.fit). None of the predictors (lagged variables and volume) appear to be statistically significant based on their p-values, as they are all above the conventional threshold of 0.05. The null deviance and residual deviance indicate the goodness of fit of the model, with the AIC providing a measure of model complexity. The null deviance (1731.2) represents the residual deviance when no predictors are included in the model, while the residual deviance (1727.6) is the deviance after fitting the predictors.\n\n\n\n\nRequirements of LDA (Linear Discriminant Analysis):\n\nLDA assumes that the predictor variables (features) follow a multivariate normal distribution within each class.\nThe classes should have approximately equal covariance matrices.\nThe classes should be linearly separable or nearly linearly separable in the feature space.\n\nDifference between LDA and Logistic Regression:\n\nLDA and logistic regression are both used for binary classification tasks, but they have different underlying assumptions and estimation methods.\nLDA assumes that the predictor variables have a multivariate normal distribution within each class and estimates the parameters based on maximizing the likelihood of observing the data given the class labels.\nLogistic regression, on the other hand, does not make assumptions about the distribution of the predictor variables and directly models the probability of the outcome using a logistic (or sigmoid) function.\nWhile LDA provides explicit probabilities for each class, logistic regression directly models the log-odds of the outcome.\n\nROC (Receiver Operating Characteristic):\n\nROC is a graphical representation of the performance of a binary classification model across different threshold values.\nIt plots the true positive rate (sensitivity) against the false positive rate (1 - specificity) for various threshold settings.\nA higher area under the ROC curve (AUC) indicates better performance of the classification model, with an AUC of 1 representing perfect classification and 0.5 indicating random guessing.\n\nSensitivity and Specificity:\n\nSensitivity (true positive rate) measures the proportion of actual positive cases that are correctly identified by the model.\nSpecificity (true negative rate) measures the proportion of actual negative cases that are correctly identified by the model.\nSensitivity is more important in scenarios where correctly identifying positive cases (e.g., detecting diseases, identifying fraudulent transactions) is critical and false negatives are costly.\nSpecificity is more important in scenarios where correctly identifying negative cases (e.g., screening out non-diseased individuals, identifying non-fraudulent transactions) is crucial and false positives are costly.\n\n\nIn my opinion, the importance of sensitivity or specificity depends on the specific context and the consequences of false positives and false negatives. In medical diagnosis, for example, sensitivity may be prioritized to minimize the risk of missing positive cases, while in spam email detection, specificity may be prioritized to minimize the risk of falsely flagging legitimate emails as spam.\nTo calculate the prediction error, we need to find the number of misclassifications, i.e., the number of instances where the predicted default status does not match the true default status.\nFrom the given table:\n\nTrue Negative (TN): 9644\nFalse Positive (FP): 252\nFalse Negative (FN): 23\nTrue Positive (TP): 81\n\nThe prediction error (PE) can be calculated using the formula:\n[ PE = {FP + FN}/{Total} ]\nLet’s plug in the values:\n[ PE = {252 + 23}/{10000} ]\n[ PE = {275}/{10000} ]\n[ PE = 0.0275 ]\nSo, the prediction error is ( 0.0275 ) or ( 2.75% )."
  },
  {
    "objectID": "6323HW7.html#lab_logisticregression01.r",
    "href": "6323HW7.html#lab_logisticregression01.r",
    "title": "Assignment 7: Logistic Regression",
    "section": "",
    "text": "require(ISLR)\n\nLoading required package: ISLR\n\n# Check dataset Smarket\n?Smarket\nnames(Smarket)\n\n[1] \"Year\"      \"Lag1\"      \"Lag2\"      \"Lag3\"      \"Lag4\"      \"Lag5\"     \n[7] \"Volume\"    \"Today\"     \"Direction\"\n\nsummary(Smarket)\n\n      Year           Lag1                Lag2                Lag3          \n Min.   :2001   Min.   :-4.922000   Min.   :-4.922000   Min.   :-4.922000  \n 1st Qu.:2002   1st Qu.:-0.639500   1st Qu.:-0.639500   1st Qu.:-0.640000  \n Median :2003   Median : 0.039000   Median : 0.039000   Median : 0.038500  \n Mean   :2003   Mean   : 0.003834   Mean   : 0.003919   Mean   : 0.001716  \n 3rd Qu.:2004   3rd Qu.: 0.596750   3rd Qu.: 0.596750   3rd Qu.: 0.596750  \n Max.   :2005   Max.   : 5.733000   Max.   : 5.733000   Max.   : 5.733000  \n      Lag4                Lag5              Volume           Today          \n Min.   :-4.922000   Min.   :-4.92200   Min.   :0.3561   Min.   :-4.922000  \n 1st Qu.:-0.640000   1st Qu.:-0.64000   1st Qu.:1.2574   1st Qu.:-0.639500  \n Median : 0.038500   Median : 0.03850   Median :1.4229   Median : 0.038500  \n Mean   : 0.001636   Mean   : 0.00561   Mean   :1.4783   Mean   : 0.003138  \n 3rd Qu.: 0.596750   3rd Qu.: 0.59700   3rd Qu.:1.6417   3rd Qu.: 0.596750  \n Max.   : 5.733000   Max.   : 5.73300   Max.   :3.1525   Max.   : 5.733000  \n Direction \n Down:602  \n Up  :648  \n           \n           \n           \n           \n\n# Create a dataframe for data browsing\nsm=Smarket\n\n# Bivariate Plot of inter-lag correlations\npairs(Smarket,col=Smarket$Direction,cex=.5, pch=20)\n\n\n\n# Logistic regression\nglm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,\n            data=Smarket,family=binomial)\nsummary(glm.fit)\n\n\nCall:\nglm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + \n    Volume, family = binomial, data = Smarket)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept) -0.126000   0.240736  -0.523    0.601\nLag1        -0.073074   0.050167  -1.457    0.145\nLag2        -0.042301   0.050086  -0.845    0.398\nLag3         0.011085   0.049939   0.222    0.824\nLag4         0.009359   0.049974   0.187    0.851\nLag5         0.010313   0.049511   0.208    0.835\nVolume       0.135441   0.158360   0.855    0.392\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1731.2  on 1249  degrees of freedom\nResidual deviance: 1727.6  on 1243  degrees of freedom\nAIC: 1741.6\n\nNumber of Fisher Scoring iterations: 3\n\nglm.probs=predict(glm.fit,type=\"response\") \nglm.probs[1:5]\n\n        1         2         3         4         5 \n0.5070841 0.4814679 0.4811388 0.5152224 0.5107812 \n\nglm.pred=ifelse(glm.probs&gt;0.5,\"Up\",\"Down\")\nattach(Smarket)\ntable(glm.pred,Direction)\n\n        Direction\nglm.pred Down  Up\n    Down  145 141\n    Up    457 507\n\nmean(glm.pred==Direction)\n\n[1] 0.5216\n\n# Make training and test set for prediction\ntrain = Year&lt;2005\nglm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,\n            data=Smarket,family=binomial, subset=train)\nglm.probs=predict(glm.fit,newdata=Smarket[!train,],type=\"response\") \nglm.pred=ifelse(glm.probs &gt;0.5,\"Up\",\"Down\")\nDirection.2005=Smarket$Direction[!train]\ntable(glm.pred,Direction.2005)\n\n        Direction.2005\nglm.pred Down Up\n    Down   77 97\n    Up     34 44\n\nmean(glm.pred==Direction.2005)\n\n[1] 0.4801587\n\n#Fit smaller model\nglm.fit=glm(Direction~Lag1+Lag2,\n            data=Smarket,family=binomial, subset=train)\nglm.probs=predict(glm.fit,newdata=Smarket[!train,],type=\"response\") \nglm.pred=ifelse(glm.probs &gt;0.5,\"Up\",\"Down\")\ntable(glm.pred,Direction.2005)\n\n        Direction.2005\nglm.pred Down  Up\n    Down   35  35\n    Up     76 106\n\nmean(glm.pred==Direction.2005)\n\n[1] 0.5595238\n\n# Check accuracy rate\n106/(76+106)\n\n[1] 0.5824176\n\n\nThis R script analyzes the “Smarket” dataset, which contains daily percentage returns for the S&P 500 stock index from 2001 to 2005, along with five predictors: “Lag1” to “Lag5” representing the previous day’s percentage returns for the S&P 500 stock index, and “Volume” representing the number of shares traded on the previous day.\n\nThe script creates a bivariate plot to visualize the inter-lag correlations. It uses the pairs() function, with colors determined by the “Direction” variable (whether the market moved up or down).\nThe script also performs logistic regression to predict the direction of the stock market movement (“Up” or “Down”) based on lagged predictors (“Lag1” to “Lag5”) and volume (“Volume”).\nThe logistic regression model’s coefficients and their significance levels are provided in the output of summary(glm.fit). None of the predictors (lagged variables and volume) appear to be statistically significant based on their p-values, as they are all above the conventional threshold of 0.05. The null deviance and residual deviance indicate the goodness of fit of the model, with the AIC providing a measure of model complexity. The null deviance (1731.2) represents the residual deviance when no predictors are included in the model, while the residual deviance (1727.6) is the deviance after fitting the predictors."
  },
  {
    "objectID": "6323HW7.html#islr-chapter-4",
    "href": "6323HW7.html#islr-chapter-4",
    "title": "Assignment 7: Logistic Regression",
    "section": "",
    "text": "Requirements of LDA (Linear Discriminant Analysis):\n\nLDA assumes that the predictor variables (features) follow a multivariate normal distribution within each class.\nThe classes should have approximately equal covariance matrices.\nThe classes should be linearly separable or nearly linearly separable in the feature space.\n\nDifference between LDA and Logistic Regression:\n\nLDA and logistic regression are both used for binary classification tasks, but they have different underlying assumptions and estimation methods.\nLDA assumes that the predictor variables have a multivariate normal distribution within each class and estimates the parameters based on maximizing the likelihood of observing the data given the class labels.\nLogistic regression, on the other hand, does not make assumptions about the distribution of the predictor variables and directly models the probability of the outcome using a logistic (or sigmoid) function.\nWhile LDA provides explicit probabilities for each class, logistic regression directly models the log-odds of the outcome.\n\nROC (Receiver Operating Characteristic):\n\nROC is a graphical representation of the performance of a binary classification model across different threshold values.\nIt plots the true positive rate (sensitivity) against the false positive rate (1 - specificity) for various threshold settings.\nA higher area under the ROC curve (AUC) indicates better performance of the classification model, with an AUC of 1 representing perfect classification and 0.5 indicating random guessing.\n\nSensitivity and Specificity:\n\nSensitivity (true positive rate) measures the proportion of actual positive cases that are correctly identified by the model.\nSpecificity (true negative rate) measures the proportion of actual negative cases that are correctly identified by the model.\nSensitivity is more important in scenarios where correctly identifying positive cases (e.g., detecting diseases, identifying fraudulent transactions) is critical and false negatives are costly.\nSpecificity is more important in scenarios where correctly identifying negative cases (e.g., screening out non-diseased individuals, identifying non-fraudulent transactions) is crucial and false positives are costly.\n\n\nIn my opinion, the importance of sensitivity or specificity depends on the specific context and the consequences of false positives and false negatives. In medical diagnosis, for example, sensitivity may be prioritized to minimize the risk of missing positive cases, while in spam email detection, specificity may be prioritized to minimize the risk of falsely flagging legitimate emails as spam.\nTo calculate the prediction error, we need to find the number of misclassifications, i.e., the number of instances where the predicted default status does not match the true default status.\nFrom the given table:\n\nTrue Negative (TN): 9644\nFalse Positive (FP): 252\nFalse Negative (FN): 23\nTrue Positive (TP): 81\n\nThe prediction error (PE) can be calculated using the formula:\n[ PE = {FP + FN}/{Total} ]\nLet’s plug in the values:\n[ PE = {252 + 23}/{10000} ]\n[ PE = {275}/{10000} ]\n[ PE = 0.0275 ]\nSo, the prediction error is ( 0.0275 ) or ( 2.75% )."
  },
  {
    "objectID": "6323HW8.html",
    "href": "6323HW8.html",
    "title": "Assignment 8: Linear Discriminant Analysis",
    "section": "",
    "text": "require(ISLR)\n\nLoading required package: ISLR\n\nrequire(MASS)\n\nLoading required package: MASS\n\nrequire(descr)\n\nLoading required package: descr\n\nattach(Smarket)\n\n## Linear Discriminant Analysis\nfreq(Direction)\n\n\n\n\nDirection \n      Frequency Percent\nDown        602   48.16\nUp          648   51.84\nTotal      1250  100.00\n\ntrain = Year&lt;2005\nlda.fit=lda(Direction~Lag1+Lag2,data=Smarket, subset=Year&lt;2005)\nlda.fit\n\nCall:\nlda(Direction ~ Lag1 + Lag2, data = Smarket, subset = Year &lt; \n    2005)\n\nPrior probabilities of groups:\n    Down       Up \n0.491984 0.508016 \n\nGroup means:\n            Lag1        Lag2\nDown  0.04279022  0.03389409\nUp   -0.03954635 -0.03132544\n\nCoefficients of linear discriminants:\n            LD1\nLag1 -0.6420190\nLag2 -0.5135293\n\nplot(lda.fit, col=\"dodgerblue\")\n\n\n\nSmarket.2005=subset(Smarket,Year==2005) # Creating subset with 2005 data for prediction\nlda.pred=predict(lda.fit,Smarket.2005)\nnames(lda.pred)\n\n[1] \"class\"     \"posterior\" \"x\"        \n\nlda.class=lda.pred$class\nDirection.2005=Smarket$Direction[!train] \ntable(lda.class,Direction.2005) \n\n         Direction.2005\nlda.class Down  Up\n     Down   35  35\n     Up     76 106\n\ndata.frame(lda.pred)[1:5,]\n\n     class posterior.Down posterior.Up         LD1\n999     Up      0.4901792    0.5098208  0.08293096\n1000    Up      0.4792185    0.5207815  0.59114102\n1001    Up      0.4668185    0.5331815  1.16723063\n1002    Up      0.4740011    0.5259989  0.83335022\n1003    Up      0.4927877    0.5072123 -0.03792892\n\ntable(lda.pred$class,Smarket.2005$Direction)\n\n      \n       Down  Up\n  Down   35  35\n  Up     76 106\n\nmean(lda.pred$class==Smarket.2005$Direction)\n\n[1] 0.5595238\n\n\nThis R script utilizes Linear Discriminant Analysis (LDA) to analyze stock market data. It first loads necessary libraries and attaches the provided dataset. Then, it fits an LDA model using lagged variables (Lag1 and Lag2) to predict the direction of the stock market before the year 2005. The script then visualizes the results of the LDA model. Next, it predicts the market direction for the year 2005 using the trained model and compares these predictions with the actual market directions. Finally, it calculates the accuracy of the model’s predictions for the year 2005. Overall, the script aims to understand whether lagged variables can effectively predict the direction of the stock market.\n\n\n\nFrom the three methods (best subset, forward stepwise, and backward stepwise), the one with the smallest training RSS is the “best subset”. The “best subset” method exhaustively searches through all possible combinations of predictors to find the model with the smallest RSS on the training data. Therefore, the model selected by the best subset method with k predictors will have the smallest training RSS among all models with k predictors.\nFrom the three methods (best subset, forward stepwise, and backward stepwise), the one with the smallest test RSS would depend. To determine the model with the smallest test RSS among the three methods, you would need to evaluate each method’s selected model on a separate test dataset. After obtaining the models selected by best subset, forward stepwise, and backward stepwise methods, you would evaluate their performance on the test dataset. The model that yields the smallest test RSS on the test dataset would have the smallest test RSS among the models selected by the three methods.\n\n\n\n\nlibrary(leaps)\nset.seed(1)\nX &lt;- rnorm(100)\nepsilon &lt;- rnorm(100)\n\nbeta0 &lt;- 4\nbeta1 &lt;- 9\nbeta2 &lt;- 2\nbeta3 &lt;- 1\n\nY &lt;- beta0 + beta1*X + beta2*X^2 + beta3*X^3 + epsilon\ndata &lt;- data.frame(Y = Y, X = X)\nbest_subset &lt;- regsubsets(Y ~ poly(X, 10, raw = TRUE), data = data, nvmax = 10)\nsummary(best_subset)\n\nSubset selection object\nCall: regsubsets.formula(Y ~ poly(X, 10, raw = TRUE), data = data, \n    nvmax = 10)\n10 Variables  (and intercept)\n                          Forced in Forced out\npoly(X, 10, raw = TRUE)1      FALSE      FALSE\npoly(X, 10, raw = TRUE)2      FALSE      FALSE\npoly(X, 10, raw = TRUE)3      FALSE      FALSE\npoly(X, 10, raw = TRUE)4      FALSE      FALSE\npoly(X, 10, raw = TRUE)5      FALSE      FALSE\npoly(X, 10, raw = TRUE)6      FALSE      FALSE\npoly(X, 10, raw = TRUE)7      FALSE      FALSE\npoly(X, 10, raw = TRUE)8      FALSE      FALSE\npoly(X, 10, raw = TRUE)9      FALSE      FALSE\npoly(X, 10, raw = TRUE)10     FALSE      FALSE\n1 subsets of each size up to 10\nSelection Algorithm: exhaustive\n          poly(X, 10, raw = TRUE)1 poly(X, 10, raw = TRUE)2\n1  ( 1 )  \"*\"                      \" \"                     \n2  ( 1 )  \"*\"                      \" \"                     \n3  ( 1 )  \"*\"                      \"*\"                     \n4  ( 1 )  \"*\"                      \"*\"                     \n5  ( 1 )  \"*\"                      \"*\"                     \n6  ( 1 )  \"*\"                      \"*\"                     \n7  ( 1 )  \"*\"                      \"*\"                     \n8  ( 1 )  \"*\"                      \"*\"                     \n9  ( 1 )  \"*\"                      \"*\"                     \n10  ( 1 ) \"*\"                      \"*\"                     \n          poly(X, 10, raw = TRUE)3 poly(X, 10, raw = TRUE)4\n1  ( 1 )  \" \"                      \" \"                     \n2  ( 1 )  \" \"                      \"*\"                     \n3  ( 1 )  \"*\"                      \" \"                     \n4  ( 1 )  \"*\"                      \" \"                     \n5  ( 1 )  \"*\"                      \" \"                     \n6  ( 1 )  \"*\"                      \" \"                     \n7  ( 1 )  \"*\"                      \" \"                     \n8  ( 1 )  \"*\"                      \"*\"                     \n9  ( 1 )  \" \"                      \"*\"                     \n10  ( 1 ) \"*\"                      \"*\"                     \n          poly(X, 10, raw = TRUE)5 poly(X, 10, raw = TRUE)6\n1  ( 1 )  \" \"                      \" \"                     \n2  ( 1 )  \" \"                      \" \"                     \n3  ( 1 )  \" \"                      \" \"                     \n4  ( 1 )  \"*\"                      \" \"                     \n5  ( 1 )  \"*\"                      \"*\"                     \n6  ( 1 )  \" \"                      \" \"                     \n7  ( 1 )  \"*\"                      \"*\"                     \n8  ( 1 )  \" \"                      \"*\"                     \n9  ( 1 )  \"*\"                      \"*\"                     \n10  ( 1 ) \"*\"                      \"*\"                     \n          poly(X, 10, raw = TRUE)7 poly(X, 10, raw = TRUE)8\n1  ( 1 )  \" \"                      \" \"                     \n2  ( 1 )  \" \"                      \" \"                     \n3  ( 1 )  \" \"                      \" \"                     \n4  ( 1 )  \" \"                      \" \"                     \n5  ( 1 )  \" \"                      \" \"                     \n6  ( 1 )  \"*\"                      \"*\"                     \n7  ( 1 )  \" \"                      \"*\"                     \n8  ( 1 )  \" \"                      \"*\"                     \n9  ( 1 )  \"*\"                      \"*\"                     \n10  ( 1 ) \"*\"                      \"*\"                     \n          poly(X, 10, raw = TRUE)9 poly(X, 10, raw = TRUE)10\n1  ( 1 )  \" \"                      \" \"                      \n2  ( 1 )  \" \"                      \" \"                      \n3  ( 1 )  \" \"                      \" \"                      \n4  ( 1 )  \" \"                      \" \"                      \n5  ( 1 )  \" \"                      \" \"                      \n6  ( 1 )  \"*\"                      \" \"                      \n7  ( 1 )  \" \"                      \"*\"                      \n8  ( 1 )  \"*\"                      \"*\"                      \n9  ( 1 )  \"*\"                      \"*\"                      \n10  ( 1 ) \"*\"                      \"*\"                      \n\nCp_values &lt;- summary(best_subset)$cp\nBIC_values &lt;- summary(best_subset)$bic\nadjR2_values &lt;- summary(best_subset)$adjr2\n\n\npar(mfrow = c(2, 2), mar = c(5, 4, 2, 1))\noptions(repr.plot.width = 10, repr.plot.height = 8)\n\n\nplot(1:10, Cp_values, type = \"b\", xlab = \"Number of Predictors\", ylab = \"Cp\", main = \"Cp Plot\")\n\n\nplot(1:10, BIC_values, type = \"b\", xlab = \"Number of Predictors\", ylab = \"BIC\", main = \"BIC Plot\")\n\n\nplot(1:10, adjR2_values, type = \"b\", xlab = \"Number of Predictors\", ylab = \"Adjusted R^2\", main = \"Adjusted R^2 Plot\")\n\n\npar(mfrow = c(1, 1), mar = c(5, 4, 4, 2) + 0.1)\n\n\n\noptions(repr.plot.width = 7, repr.plot.height = 5)\n\n\nBest model according to Cp and BIC: Model with 9 predictors.\nBest model according to adjusted R^2: Model with 6 predictors."
  },
  {
    "objectID": "6323HW8.html#lab_lda01.r",
    "href": "6323HW8.html#lab_lda01.r",
    "title": "Assignment 8: Linear Discriminant Analysis",
    "section": "",
    "text": "require(ISLR)\n\nLoading required package: ISLR\n\nrequire(MASS)\n\nLoading required package: MASS\n\nrequire(descr)\n\nLoading required package: descr\n\nattach(Smarket)\n\n## Linear Discriminant Analysis\nfreq(Direction)\n\n\n\n\nDirection \n      Frequency Percent\nDown        602   48.16\nUp          648   51.84\nTotal      1250  100.00\n\ntrain = Year&lt;2005\nlda.fit=lda(Direction~Lag1+Lag2,data=Smarket, subset=Year&lt;2005)\nlda.fit\n\nCall:\nlda(Direction ~ Lag1 + Lag2, data = Smarket, subset = Year &lt; \n    2005)\n\nPrior probabilities of groups:\n    Down       Up \n0.491984 0.508016 \n\nGroup means:\n            Lag1        Lag2\nDown  0.04279022  0.03389409\nUp   -0.03954635 -0.03132544\n\nCoefficients of linear discriminants:\n            LD1\nLag1 -0.6420190\nLag2 -0.5135293\n\nplot(lda.fit, col=\"dodgerblue\")\n\n\n\nSmarket.2005=subset(Smarket,Year==2005) # Creating subset with 2005 data for prediction\nlda.pred=predict(lda.fit,Smarket.2005)\nnames(lda.pred)\n\n[1] \"class\"     \"posterior\" \"x\"        \n\nlda.class=lda.pred$class\nDirection.2005=Smarket$Direction[!train] \ntable(lda.class,Direction.2005) \n\n         Direction.2005\nlda.class Down  Up\n     Down   35  35\n     Up     76 106\n\ndata.frame(lda.pred)[1:5,]\n\n     class posterior.Down posterior.Up         LD1\n999     Up      0.4901792    0.5098208  0.08293096\n1000    Up      0.4792185    0.5207815  0.59114102\n1001    Up      0.4668185    0.5331815  1.16723063\n1002    Up      0.4740011    0.5259989  0.83335022\n1003    Up      0.4927877    0.5072123 -0.03792892\n\ntable(lda.pred$class,Smarket.2005$Direction)\n\n      \n       Down  Up\n  Down   35  35\n  Up     76 106\n\nmean(lda.pred$class==Smarket.2005$Direction)\n\n[1] 0.5595238\n\n\nThis R script utilizes Linear Discriminant Analysis (LDA) to analyze stock market data. It first loads necessary libraries and attaches the provided dataset. Then, it fits an LDA model using lagged variables (Lag1 and Lag2) to predict the direction of the stock market before the year 2005. The script then visualizes the results of the LDA model. Next, it predicts the market direction for the year 2005 using the trained model and compares these predictions with the actual market directions. Finally, it calculates the accuracy of the model’s predictions for the year 2005. Overall, the script aims to understand whether lagged variables can effectively predict the direction of the stock market."
  },
  {
    "objectID": "6323HW8.html#islr-chapter-6",
    "href": "6323HW8.html#islr-chapter-6",
    "title": "Assignment 8: Linear Discriminant Analysis",
    "section": "",
    "text": "From the three methods (best subset, forward stepwise, and backward stepwise), the one with the smallest training RSS is the “best subset”. The “best subset” method exhaustively searches through all possible combinations of predictors to find the model with the smallest RSS on the training data. Therefore, the model selected by the best subset method with k predictors will have the smallest training RSS among all models with k predictors.\nFrom the three methods (best subset, forward stepwise, and backward stepwise), the one with the smallest test RSS would depend. To determine the model with the smallest test RSS among the three methods, you would need to evaluate each method’s selected model on a separate test dataset. After obtaining the models selected by best subset, forward stepwise, and backward stepwise methods, you would evaluate their performance on the test dataset. The model that yields the smallest test RSS on the test dataset would have the smallest test RSS among the models selected by the three methods."
  },
  {
    "objectID": "6323HW8.html#application-exercise",
    "href": "6323HW8.html#application-exercise",
    "title": "Assignment 8: Linear Discriminant Analysis",
    "section": "",
    "text": "library(leaps)\nset.seed(1)\nX &lt;- rnorm(100)\nepsilon &lt;- rnorm(100)\n\nbeta0 &lt;- 4\nbeta1 &lt;- 9\nbeta2 &lt;- 2\nbeta3 &lt;- 1\n\nY &lt;- beta0 + beta1*X + beta2*X^2 + beta3*X^3 + epsilon\ndata &lt;- data.frame(Y = Y, X = X)\nbest_subset &lt;- regsubsets(Y ~ poly(X, 10, raw = TRUE), data = data, nvmax = 10)\nsummary(best_subset)\n\nSubset selection object\nCall: regsubsets.formula(Y ~ poly(X, 10, raw = TRUE), data = data, \n    nvmax = 10)\n10 Variables  (and intercept)\n                          Forced in Forced out\npoly(X, 10, raw = TRUE)1      FALSE      FALSE\npoly(X, 10, raw = TRUE)2      FALSE      FALSE\npoly(X, 10, raw = TRUE)3      FALSE      FALSE\npoly(X, 10, raw = TRUE)4      FALSE      FALSE\npoly(X, 10, raw = TRUE)5      FALSE      FALSE\npoly(X, 10, raw = TRUE)6      FALSE      FALSE\npoly(X, 10, raw = TRUE)7      FALSE      FALSE\npoly(X, 10, raw = TRUE)8      FALSE      FALSE\npoly(X, 10, raw = TRUE)9      FALSE      FALSE\npoly(X, 10, raw = TRUE)10     FALSE      FALSE\n1 subsets of each size up to 10\nSelection Algorithm: exhaustive\n          poly(X, 10, raw = TRUE)1 poly(X, 10, raw = TRUE)2\n1  ( 1 )  \"*\"                      \" \"                     \n2  ( 1 )  \"*\"                      \" \"                     \n3  ( 1 )  \"*\"                      \"*\"                     \n4  ( 1 )  \"*\"                      \"*\"                     \n5  ( 1 )  \"*\"                      \"*\"                     \n6  ( 1 )  \"*\"                      \"*\"                     \n7  ( 1 )  \"*\"                      \"*\"                     \n8  ( 1 )  \"*\"                      \"*\"                     \n9  ( 1 )  \"*\"                      \"*\"                     \n10  ( 1 ) \"*\"                      \"*\"                     \n          poly(X, 10, raw = TRUE)3 poly(X, 10, raw = TRUE)4\n1  ( 1 )  \" \"                      \" \"                     \n2  ( 1 )  \" \"                      \"*\"                     \n3  ( 1 )  \"*\"                      \" \"                     \n4  ( 1 )  \"*\"                      \" \"                     \n5  ( 1 )  \"*\"                      \" \"                     \n6  ( 1 )  \"*\"                      \" \"                     \n7  ( 1 )  \"*\"                      \" \"                     \n8  ( 1 )  \"*\"                      \"*\"                     \n9  ( 1 )  \" \"                      \"*\"                     \n10  ( 1 ) \"*\"                      \"*\"                     \n          poly(X, 10, raw = TRUE)5 poly(X, 10, raw = TRUE)6\n1  ( 1 )  \" \"                      \" \"                     \n2  ( 1 )  \" \"                      \" \"                     \n3  ( 1 )  \" \"                      \" \"                     \n4  ( 1 )  \"*\"                      \" \"                     \n5  ( 1 )  \"*\"                      \"*\"                     \n6  ( 1 )  \" \"                      \" \"                     \n7  ( 1 )  \"*\"                      \"*\"                     \n8  ( 1 )  \" \"                      \"*\"                     \n9  ( 1 )  \"*\"                      \"*\"                     \n10  ( 1 ) \"*\"                      \"*\"                     \n          poly(X, 10, raw = TRUE)7 poly(X, 10, raw = TRUE)8\n1  ( 1 )  \" \"                      \" \"                     \n2  ( 1 )  \" \"                      \" \"                     \n3  ( 1 )  \" \"                      \" \"                     \n4  ( 1 )  \" \"                      \" \"                     \n5  ( 1 )  \" \"                      \" \"                     \n6  ( 1 )  \"*\"                      \"*\"                     \n7  ( 1 )  \" \"                      \"*\"                     \n8  ( 1 )  \" \"                      \"*\"                     \n9  ( 1 )  \"*\"                      \"*\"                     \n10  ( 1 ) \"*\"                      \"*\"                     \n          poly(X, 10, raw = TRUE)9 poly(X, 10, raw = TRUE)10\n1  ( 1 )  \" \"                      \" \"                      \n2  ( 1 )  \" \"                      \" \"                      \n3  ( 1 )  \" \"                      \" \"                      \n4  ( 1 )  \" \"                      \" \"                      \n5  ( 1 )  \" \"                      \" \"                      \n6  ( 1 )  \"*\"                      \" \"                      \n7  ( 1 )  \" \"                      \"*\"                      \n8  ( 1 )  \"*\"                      \"*\"                      \n9  ( 1 )  \"*\"                      \"*\"                      \n10  ( 1 ) \"*\"                      \"*\"                      \n\nCp_values &lt;- summary(best_subset)$cp\nBIC_values &lt;- summary(best_subset)$bic\nadjR2_values &lt;- summary(best_subset)$adjr2\n\n\npar(mfrow = c(2, 2), mar = c(5, 4, 2, 1))\noptions(repr.plot.width = 10, repr.plot.height = 8)\n\n\nplot(1:10, Cp_values, type = \"b\", xlab = \"Number of Predictors\", ylab = \"Cp\", main = \"Cp Plot\")\n\n\nplot(1:10, BIC_values, type = \"b\", xlab = \"Number of Predictors\", ylab = \"BIC\", main = \"BIC Plot\")\n\n\nplot(1:10, adjR2_values, type = \"b\", xlab = \"Number of Predictors\", ylab = \"Adjusted R^2\", main = \"Adjusted R^2 Plot\")\n\n\npar(mfrow = c(1, 1), mar = c(5, 4, 4, 2) + 0.1)\n\n\n\noptions(repr.plot.width = 7, repr.plot.height = 5)\n\n\nBest model according to Cp and BIC: Model with 9 predictors.\nBest model according to adjusted R^2: Model with 6 predictors."
  }
]